{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "**SVM Definition (rough) ** - it finds data of two different classes and a support vectory machine \n",
    "outputs a line which separates the two classes. \n",
    "\n",
    "### Example 1:  \n",
    "<img src=\"svm_images/example_1.png\" alt = \"SVM example\" style =\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "### Margin \n",
    "* Maximize the distances of a line $M$ to nearest point $(a,b) \\in \\mathbb{R}$, such that, $|(a,b) - M|$. \n",
    "  This is refered to as the **Margin**. \n",
    "  \n",
    "### Example 2: \n",
    "<img src=\"svm_images/example_2.png\" alot = \"Margin\" style = \"width: 500px;\"/> \n",
    "Essentially we want to maximazie the robustness of your result. Hence, by chosing the maximize distance of a line \n",
    "$M$ we get a better result with less noise. \n",
    "\n",
    "\n",
    "## SVMs - Outliers \n",
    "\n",
    "### Example 3:\n",
    "Sometimes you might have a data set below where nothing will seperate the two classes. Essentially, \n",
    "we just want to do the best you can of the outlier point. Tolerate the individual outliers. This mediates the seperator and ignore the outliers in (SVM). \n",
    "<img src=\"svm_images/example_3.png\" alt = \"Outliers\" style =\"width: 250px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training features': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)}\n",
      "{'your prediction is': array([1])}\n"
     ]
    }
   ],
   "source": [
    "# coding with sklearn \n",
    "# sample code \n",
    "from sklearn import svm #import svm\n",
    "\n",
    "X = [[0,0], [1,1]] #training features\n",
    "Y = [0,1] #training labels \n",
    "clf = svm.SVC() #classifier \n",
    "print {\"training features\": clf.fit(X,Y)} #fit using training features\n",
    "\n",
    "\"\"\"After being fitted, the model can then be used to predict new values.\"\"\"\n",
    "print {\"your prediction is\":clf.predict([[2.,2.]])} #prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of Chris training emails: 7936\n",
      "no. of Sara training emails: 7884\n",
      "0.984072810011\n"
     ]
    }
   ],
   "source": [
    "#Quiz \n",
    "import sys \n",
    "from time import time\n",
    "sys.path.append(\"../tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "########################## SVM #################################\n",
    "### we handle the import statement and SVC creation for you here\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel=\"linear\")\n",
    "clf.fit(features_train, labels_train) # create a fit. \n",
    "\n",
    "pred = clf.predict(features_test) # prediction from test. \n",
    "\n",
    "from sklearn.metrics import accuracy_score #import accuracy.\n",
    "acc = accuracy_score(pred, labels_test) #create accuracy\n",
    "\n",
    "def submitAccuracy():\n",
    "    return acc\n",
    "\n",
    "print submitAccuracy() #should return 98% accuracy! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Linear SVM \n",
    "\n",
    "### Features \n",
    "so far we have assumed our features $(x,y)$ to have linear representation. We placed our \n",
    "features into a training $SVM$ and we output some label. For non-linear represenation we \n",
    "will be following a more eleptical space like $x^2 + y^2 = z$. $z$ is the hyperplane representation of the point. \n",
    "Within this plane we may find linear represenations.\n",
    "\n",
    "### Example 1: \n",
    "<img src=\"svm_images/example_4.png\" alt = \"Linear Seperation\" style =\"width: 500px;\"/> \n",
    "\n",
    "all the values of the \"circle = $c$\" and \"exes = $\\hat{x}$\" take on some distance in the hyperplane where \n",
    "all the $\\hat{x}$ take on a small value of $f(z)$ and all the $c$ take on a large value or $\\hat{x} < c \\in f(z)$. This is shown below \n",
    "\n",
    "### Example 2: \n",
    "\n",
    "<img src=\"svm_images/example_5.png\" alt = \"z-plane\" style =\"width: 300px;\"/> \n",
    "\n",
    "Therefore, we have linear seperation in the hyperplane. \n",
    "\n",
    "### Kernel Trick \n",
    "\n",
    "Essentially we take two points $(x,y) \\in \\mathbb{R}$ and map them into $(x,y) \\in \\mathbb{R}^d$ were \n",
    "$d$ is the multi dimensional space. $(x,y)$ are not seperable inside $\\mathbb{R}$ but are separable \n",
    "in the multi dimensional space $\\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters in ML\n",
    "\n",
    "Definition: Parameters are arguments passed when you create your classifier. \n",
    "* Before fitting\n",
    "\n",
    " \n",
    "These can make a **DIFFERENCE!!** in the decision boundary. \n",
    "\n",
    "## What are parameters for SVM?\n",
    "\n",
    "* Kernel \n",
    "* C \n",
    "* Gamma \n",
    "<img src=\"svm_images/example_6.png\" alt = \"parameters\" style =\"width: 500px;\"/> \n",
    "\n",
    "## SVM C Parameter\n",
    "\n",
    "**C** - controls tradeoff between smooth decisions boundary and classifying training points correctly. \n",
    "\n",
    "### Quiz: \n",
    "Does a large C mean you expect a smooth boundary, or that you will get more training points correct? \n",
    "**ANSWER:** A low ``c`` makes the decision surface smooth, while a high ``c`` aims at classifying all training examples by giving the model freedom to select more samples as support vectors. So the answer is **more training points correct**\n",
    "\n",
    "### SVM Gamma Parameter \n",
    "\n",
    "**$\\gamma$** - defines how far teh influence of a single training examples reaches\n",
    "* low values - far reach \n",
    "* high values - close reach \n",
    "\n",
    "<img src=\"svm_images/example_7.png\" alt = \"gamma\" style =\"width: 500px;\"/> \n",
    "\n",
    "\n",
    "### Quiz: Overfitting\n",
    "\n",
    "We really want to avoid overfitting. This is through the paramter of your algorithm. \n",
    "any paramteres make you more or less of overfitting your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
