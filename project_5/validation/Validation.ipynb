{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation \n",
    "\n",
    "\n",
    "### Why Use Training & Testing Data? \n",
    "\n",
    "- Gives estimate of performance on an independent dataset\n",
    " \n",
    "- Serves as check on overfitting \n",
    "\n",
    "\n",
    "## sklearn train test split \n",
    "Here is an exampled of how to implement cross validation: \n",
    "``` python \n",
    "    import numpy as np \n",
    "    from sklearn import cross_validation \n",
    "    from sklearn import datasets \n",
    "    from sklearn import svm \n",
    "    \n",
    "    \n",
    "    # Load dataset \n",
    "    iris = datasets.load_iris()\n",
    "   \n",
    "   \n",
    "    # create a quick sample training set while holding our 40% of the data\n",
    "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "    iris.data, iris.target, text_size=0.4, random_state=0) \n",
    "    \n",
    "    # create an event\n",
    "    X_train.shape, y_train.shape ## ((90,4), (90,))\n",
    "    X_test.shape, y_test.shape ## ((60,4), (60,))\n",
    "    \n",
    "    # classifier \n",
    "    clf = svm.SVC(kernel = \"linear\", C=1).fit(X_train,y_train) \n",
    "    \n",
    "    # evaluation of test set\n",
    "    clf.score(X_test,y_test) # 0.96 . . . \n",
    "```\n",
    "\n",
    "## Training, Transforms, Predicting \n",
    "\n",
    "Suppose your overall example looks something like this \n",
    "\n",
    "$$ \\textbf{Train & Test_split} \\Rightarrow  PCA \\Rightarrow SVM$$\n",
    "\n",
    "remember $PCA$ has a few commands on it: \n",
    "\n",
    "- $PCA$:   ```pca.fit``` & ```pca.transform``` \n",
    "- $SVC$: ```svc.fit ``` & ```svc.predict``` \n",
    "\n",
    "\n",
    "- ```pca.fit(training-features)``` we only want to look for patterns in the training data\n",
    "- ```pca.transform(training-features) ``` in order to use principle components we need to transform into the correct rep.\n",
    "- ```svc.train(training-features) ``` \n",
    "\n",
    "The training of the classifier is done now we have to do testing features therefore, \n",
    "\n",
    "- ```pca.transform(test_features) ``` \n",
    "- ```svc.predict(test_features) ``` \n",
    "\n",
    "you wanna make your predictions making the test dataset this is the whole point\n",
    "\n",
    "\n",
    "## Cross-Validation \n",
    "\n",
    "**Problems with splitting into training and testing data:** \n",
    "\n",
    "suppose this is your data: \n",
    "\n",
    "<img src= \"v_images/example_1.png\" style=\"width: 500px;\"/> \n",
    "\n",
    "you would like to maximize both of the sets maximum in test set to get best validation. You arrive at some lossesness \n",
    "\n",
    "### K-fold cross validation \n",
    "\n",
    "Run $k$ seperate learning experiements \n",
    "\n",
    "- pick testing set \n",
    "- remaining $k-1$ training set \n",
    "- test on the testing set\n",
    "\n",
    "Average the testing results from the $k$ experiemnts. \n",
    "\n",
    "### Practical advice for k-fold in Sklearn \n",
    "\n",
    "If our original data comes in some sort of sorted fashion, then we will want to first shuffle the order of the data points before splitting them up into folds, or otherwise randomly assign data points to each fold. If we want to do this using ```KFold()```, then we can add the \"shuffle = True\" parameter when setting up the cross-validation object.\n",
    "\n",
    "If we have concerns about class imbalance, then we can use the ```StratifiedKFold()``` class instead. Where ```KFold()``` assigns points to folds without attention to output class, ```StratifiedKFold()``` assigns data points to folds so that each fold has approximately the same number of data points of each output class. This is most useful for when we have imbalanced numbers of data points in your outcome classes (e.g. one is rare compared to the others). For this class as well, we can use \"shuffle = True\" to shuffle the data points' order before splitting into folds.\n",
    "\n",
    "\n",
    "## GridSearch CV in sklearn \n",
    "\n",
    "GridSearchCV is a way of systematically working through multiple combinations of parameter tunes, cross-validating as it goes to determine which tune gives the best performance. The beauty is that it can work through many combinations in only a couple extra lines of code.\n",
    "\n",
    "Here's an example from the sklearn documentation:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html\n",
    "\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "clf = grid_search.GridSearchCV(svr, parameters)\n",
    "clf.fit(iris.data, iris.target)\n",
    "\n",
    "Let's break this down line by line.\n",
    "\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "\n",
    "A dictionary of the parameters, and the possible values they may take. In this case, they're playing around with the kernel (possible choices are 'linear' and 'rbf'), and C (possible choices are 1 and 10).\n",
    "\n",
    "Then a 'grid' of all the following combinations of values for (kernel, C) are automatically generated:\n",
    "\n",
    "('rbf', 1)\t('rbf', 10)\n",
    "('linear', 1)\t('linear', 10)\n",
    "\n",
    "\n",
    "Each is used to train an SVM, and the performance is then assessed using cross-validation.\n",
    "\n",
    "svr = svm.SVC() \n",
    "This looks kind of like creating a classifier, just like we've been doing since the first lesson. But note that the \"clf\" isn't made until the next line--this is just saying what kind of algorithm to use. Another way to think about this is that the \"classifier\" isn't just the algorithm in this case, it's algorithm plus parameter values. Note that there's no monkeying around with the kernel or C; all that is handled in the next line.\n",
    "\n",
    "clf = grid_search.GridSearchCV(svr, parameters) \n",
    "This is where the first bit of magic happens; the classifier is being created. We pass the algorithm (svr) and the dictionary of parameters to try (parameters) and it generates a grid of parameter combinations to try.\n",
    "\n",
    "clf.fit(iris.data, iris.target) \n",
    "And the second bit of magic. The fit function now tries all the parameter combinations, and returns a fitted classifier that's automatically tuned to the optimal parameter combination. You can now access the parameter values via clf.best_params_.\n",
    "\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n",
      "0.973684210526\n",
      "0.973684210526\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'authors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-787e59ac06e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mkf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'authors' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets \n",
    "from sklearn.svm import SVC \n",
    "\n",
    "# load data set\n",
    "iris = datasets.load_iris() \n",
    "features = iris.data \n",
    "labels = iris.target \n",
    "\n",
    "# get shape of features , labels \n",
    "\n",
    "print features.shape\n",
    "print labels.shape\n",
    "\n",
    "\n",
    "from sklearn import cross_validation \n",
    "\n",
    "features_train,features_test,labels_train,labels_test = cross_validation.train_test_split(\n",
    "features, labels,  random_state = 0 )\n",
    "\n",
    "\n",
    "clf = SVC(kernel=\"linear\", C=1.)\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "print clf.score(features_test, labels_test)\n",
    "##############################################################\n",
    "def submitAcc():\n",
    "    return clf.score(features_test, labels_test)\n",
    "\n",
    "\n",
    "print submitAcc() \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# k-fold cross validation \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
