{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "### Example 1: \n",
    "\n",
    "$x$ indicates he's not windsurfing and $o$ otherwise. This image is not $LS$. \n",
    "\n",
    "<img src=\"dt_images/example_1.png\" alot = \"Wind-Surfing\" style = \"width: 500px;\"/> \n",
    "\n",
    "*Decision Trees* let's you ask multiple questions. Like is it Windy (Yes, No)? If it is Windy, we need to create another decision. This leads us to some sort of tree graph as described below \n",
    "\n",
    "### Example 2: \n",
    "\n",
    "<img src=\"dt_images/example_2.png\" alot = \"Wind-Surfing-Decision\" style = \"width: 500px;\"/> \n",
    "\n",
    "**Decision Trees** helps us classify data charectorizing it as decision of questions.\n",
    "\n",
    "### Example 3: \n",
    "\n",
    "<img src=\"dt_images/example_3.png\" alot = \"Decisions-Equalities\" style = \"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Classifier and Prediction': (DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), array([1]))}\n"
     ]
    }
   ],
   "source": [
    "# Decision Trees code example . \n",
    "from sklearn import tree #import dt\n",
    "X = [[0,0], [1,1]] #training features\n",
    "Y = [0,1] # training labels\n",
    "clf = tree.DecisionTreeClassifier() # create classifier \n",
    "clf = clf.fit(X,Y) #fit function\n",
    "pred = clf.predict([[2.,2.]])\n",
    "\n",
    "print{\"Classifier and Prediction\": (clf, pred)} # output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# min_samples_split \n",
    "\n",
    "Essentially, what split achieves is taking one node and spliting it by $n$ choices. Here below \n",
    "$n = 2$, so each node keeps spliting until $n \\Rightarrow 1$. \n",
    "\n",
    "### Example 4: \n",
    "<img src=\"dt_images/example_4.png\" alot = \"mss\" style = \"width: 400px;\"/>\n",
    "The more splits you have the more packed your data becomes which causes overfitting, which came be problematic in some cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min_samples_split and Overfitting\n",
    "\n",
    "### Example 5: \n",
    "\n",
    "<img src=\"dt_images/example_5.png\" alot = \"mss-overfitting\" style = \"width: 500px;\"/>\n",
    "\n",
    "one of them is min_samlpes split $= 2$ and one $= 50$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn import tree \\nfrom sklearn.metrics import accuracy_score\\nclf_50 = tree.DecisionTreeClassifier(min_samples_split =50) \\nclf_50 = clf_50.fit(features_train,labels_train) \\npred = clf_50.predict(features_test)\\nacc_min_samples_split_50 = accuracy_score(labels_test, pred)\\n\\nclf_2 = tree.DecisionTreeClassifier(min_samples_split =2) \\nclf_2 = clf_2.fit(features_train,labels_train) \\npred = clf_2.predict(features_test)\\nacc_min_samples_split_2 = accuracy_score(labels_test, pred)\\n\\n\\ndef submitAccuracies():\\n  return {\"acc_min_samples_split_2\":round(acc_min_samples_split_2,3),\\n          \"acc_min_samples_split_50\":round(acc_min_samples_split_50,3)} \\n\\n\\nHere\\'s your output:\\n{\\'acc_min_samples_split_50\\': 0.912, \\'acc_min_samples_split_2\\': 0.908}\\n          \\n          '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example code\n",
    "\n",
    "\"\"\"from sklearn import tree \n",
    "from sklearn.metrics import accuracy_score\n",
    "clf_50 = tree.DecisionTreeClassifier(min_samples_split =50) \n",
    "clf_50 = clf_50.fit(features_train,labels_train) \n",
    "pred = clf_50.predict(features_test)\n",
    "acc_min_samples_split_50 = accuracy_score(labels_test, pred)\n",
    "\n",
    "clf_2 = tree.DecisionTreeClassifier(min_samples_split =2) \n",
    "clf_2 = clf_2.fit(features_train,labels_train) \n",
    "pred = clf_2.predict(features_test)\n",
    "acc_min_samples_split_2 = accuracy_score(labels_test, pred)\n",
    "\n",
    "\n",
    "def submitAccuracies():\n",
    "  return {\"acc_min_samples_split_2\":round(acc_min_samples_split_2,3),\n",
    "          \"acc_min_samples_split_50\":round(acc_min_samples_split_50,3)} \n",
    "\n",
    "\n",
    "Here's your output:\n",
    "{'acc_min_samples_split_50': 0.912, 'acc_min_samples_split_2': 0.908}\n",
    "          \n",
    "          \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy \n",
    "\n",
    "Def: Entropy controlls how a DT decides where to split the data and measures of impurity in a bunch of examples. \n",
    "\n",
    "### Example 6: \n",
    "<img src=\"dt_images/example_6.png\" alot = \"entropy\" style = \"width: 500px;\"/>\n",
    "\n",
    "Essentially, what we would like to achieve is more purity within the examples of the data set. Variables and split points that will create subsets that are as pure as possible.\n",
    "\n",
    "entropy $E$ is mathematically defined as: \n",
    "$$ E = -\\sum_{i}p_{i}log_{2}(p_{i}) $$\n",
    "\n",
    "* $p_{i}$ is the fraction of examples in class $i$\n",
    "* $\\sum_{i}$ is the sum over all classes available \n",
    "\n",
    "### Example 7: \n",
    "<img src=\"dt_images/example_7.png\" alot = \"intution\" style = \"width: 500px;\"/>\n",
    "\n",
    "\n",
    "### Example 8: \n",
    "<img src=\"dt_images/example_8.png\" alot = \"example_e\" style = \"width: 500px;\"/> \n",
    "\n",
    "* $p_{slow} = $ the fraction of slow examples = $\\frac{ss}{ssff}$ or $\\frac{1}{2}$ \n",
    "* $p_{fast} = $ the fraction of fast examples = $\\frac{ff}{ssff}$ or $\\frac{1}{2}$\n",
    "\n",
    "Puting this all together we shouldd have: \n",
    "\n",
    "$$ p_{slow}log_{2}(p_{slow}) + p_{fast}log_{2}(p_{fast}) $$ \n",
    "\n",
    "or simply put: \n",
    "\n",
    "$$ E_{ssff} = -(0.5*log_{2}(0.5) + 0.5*log_{2}(0.5)) = 1.0$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to use the entropy formula \n",
    "import math as m \n",
    "E_ssff = -(0.5*m.log(0.5,2) + 0.5*m.log(0.5,2))\n",
    "print E_ssff\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Gain \n",
    "\n",
    "### Example 9: \n",
    "<img src=\"dt_images/example_9.png\" alot = \"example_e\" style = \"width: 500px;\"/> \n",
    "\n",
    "The decision tree will **maximize information gain** \n",
    "\n",
    "* entropy of parent = 1.0 \n",
    "* entropy of children = 3/4(0.9184) + 1/4(0) weighted average of nodes\n",
    "* information gain = 1.0 - [3/4(0.9184) + 1/4(0)] 31% \n",
    "\n",
    "This is the information gain we get if we split on the grade. \n",
    "\n",
    "* entropy of parent = 1.0 \n",
    "* entropy of bumpy = $-(p_{bs}*log_{2}(p_{bs}) + p_{bf}*log_{2}(p_{bf}))$ \n",
    "* entropy of smooth = $-(p_{ss}*log_{2}(p_{ss}) + p_{sf}*log_{2}(p_{sf}))$ \n",
    "* information gain = $E_{parent} - [1/2*E_{bumpy} + 1/2*E_{smooth}] = 0$\n",
    "\n",
    "Split based on speed limit \n",
    "* entropy of parent = 1.0 \n",
    "* entropy of yes = $E_{yes} = -(p_{ss}*log_{2}(p_{ss}) + p_{ff}*log_{2}(p_{ff})) = 0$\n",
    "* entropy of no =  $E_{no} = -(p_{ss}*log_{2}(p_{ss}) + p_{ff}*log_{2}(p_{ff})) = 0$\n",
    "* information gain = $E_{parent} - [1/2*E_{yes} + 1/2*E_{no}] = 1.0$ \n",
    "\n",
    "so if we start of with an entropy from 1 to 1 that's the split we want to use for the decision tree this is very importy. Caluclations like this is hat the decision tree does for all the changens to make splits in the data set. \n",
    "\n",
    "#### gini index \n",
    "support \"entropy\" for the information gain. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "Decision trees are prone to overfitting, especially when you have alot of features it can overfit with the datza you need to be careful with the parameter. Decision Trees can build bigger classifer with ensemble methods. Now we will decide who wrote an email bassed on the written emails on decision trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
