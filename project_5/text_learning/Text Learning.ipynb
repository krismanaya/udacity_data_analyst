{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Learning\n",
    "\n",
    "Suppose we have two strings \"Nice Day\" and \"A Very Nice Day\" one indicated as $[0,1]$.\n",
    "\n",
    "<img src=\"t_images/example_1.png\" alt=\"text learning\" style= \"width: 500px;\" /> \n",
    "\n",
    "**Problems** \n",
    "the lenght of the document is non-standardize. instead we use bag of words. \n",
    "\n",
    "\n",
    "### Bag of Words\n",
    "\n",
    "Take any text and count the frequency of words: \n",
    "\n",
    "<img src=\"t_images/example_2.png\" alt=\"text learning\" style= \"width: 500px;\" /> \n",
    "\n",
    "### Vocabulary: Not all words are equal \n",
    "\n",
    "**some words contain more information than other** \n",
    "- check the low-information words\n",
    "- stopwords: and, the, I, you, have [low information highly frequency words]\n",
    "\n",
    "suppose our **stopwords - [the,in,for,you,will,have,be]** how many should be low info in the message: \n",
    "**\"Hi Katie the machine learning class will be great best Sebastian\"** \n",
    "\n",
    "\n",
    "### Vocabulary: Not all unique words are different \n",
    "\n",
    "say in corpus we have **unresponsive, response, responsivity, responsiveness, respond ** most of these words are fairly similary so we need to wrap them into a algorithm called a **Stemmer** \n",
    "\n",
    "**Stemmer -> respon [which is the stem of a word]** so we take a five dimensional space into one dimension.\n",
    "\n",
    "we have learned\n",
    "\n",
    "* Bag of words\n",
    "* words stemming (stem each word first then put it into the representation in the ML algorithm)\n",
    "\n",
    "### TFIDF Representation \n",
    "\n",
    "TF - Term Frequency\n",
    "IDF - Inverse document frequency\n",
    "\n",
    "TF : each term each word will be weighted as it occurs in the document\n",
    "IDF: the word also gets a weighting in the corpus as a whole. \n",
    "\n",
    "What this does is it rates the rare words more commonely then the high ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 16)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 6)\t3\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 15)\t1\n",
      "  (1, 16)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 15)\t1\n",
      "  (2, 16)\t1\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Bag of words sklearn (AKA Count Vectorizer)\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pprint\n",
    "vectorizer = CountVectorizer() # vectorizer\n",
    "string1 = \"hi Katie the self driving car will be late Best Sebastian\"\n",
    "string2 = \"Hi Sebastian the machine learning class will be great great great Best Katie\"\n",
    "string3 = \"Hi Katie the machine learning class will be most excellent\"\n",
    "email_list = [string1,string2,string3] # create a list\n",
    "\n",
    "bag_of_words = vectorizer.fit(email_list) # fit the email list \n",
    "bag_of_words = vectorizer.transform(email_list) # transform it into the bag of words\n",
    "\n",
    "print bag_of_words\n",
    "#  (1, 7)\t1 example row \n",
    "#  in document #1 word number seven occurs one time.\n",
    "#  in document #1 word number six occurs three times (\"great great great\")\n",
    "\n",
    "print vectorizer.vocabulary_.get(\"great\") # prints the tuple (1,6) --> 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "me\n",
      "my\n",
      "myself\n",
      "we\n",
      "our\n",
      "ours\n",
      "ourselves\n",
      "you\n",
      "your\n",
      "yours\n",
      "153\n"
     ]
    }
   ],
   "source": [
    "# NLTK Getting stopwords. \n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words(\"english\") #checks all the stopwords in the english language\n",
    "for i in range(0,11): \n",
    "    print sw[i]\n",
    "print len(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download() \n",
    "\n",
    "# stemmer \n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmer.stem(\"responsiveness\") # the stem is \"respons\"\n",
    "stemmer.stem(\"responsivity\") # same as before\n",
    "stemmer.stem(\"unresponsive\") # the stem is \"unrespons\"'\n",
    "stemmer.stem(\"hi\")\n",
    "stem_test = \"hi everyone everyone if you can read this message youre properly using parseouttext  please proceed to the next part of the project\"\n",
    "stem_test_list = stem_test.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = \"\"\n",
    "newstr = words.join(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi everyone everyone if you can read this message youre properly using parseouttext  please proceed to the next part of the project\n",
      "hi   if you can read this message youre properly using parseouttext  please proceed to the next part of the project\n"
     ]
    }
   ],
   "source": [
    "remove = [\"everyone\", \"if\", \"message\"]\n",
    "print stem_test\n",
    "print stem_test.replace(\"everyone\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
