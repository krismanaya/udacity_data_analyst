{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis - PCA\n",
    "\n",
    "\n",
    "PCA - if you are given data of any shape PCA finds a new coordinate system by translation and rotation it moves the x-axis into the most variation of the data points. \n",
    "\n",
    "<img src= \"p_images/example_1.png\" alt= \"PCA\" style=\"width: 500px;\"/> \n",
    "\n",
    "essentially we create to vectors at the center of the data which are orthognal to each other.This is known as a transformation. Given two vectors $\\vec{y}$ and $\\vec{x}$ we must ensure that at some new point $P$ these vectors our orthognal $\\vec{y} \\perp \\vec{x}$\n",
    "\n",
    "\n",
    "## Measurable vs. Latent Features\n",
    "\n",
    "\n",
    "question: given the features of a house, what is its price? this is a classic regression exercise so it's a conyinous which is a number as an output. \n",
    "\n",
    "** Measurable ** \n",
    " - square footage \n",
    " - no. of rooms \n",
    " - school ranking \n",
    " - neighborhood safety \n",
    "\n",
    "we arre proping two things (size, neighborhood) you can't measure directly but are drvigin the phenomenon behind the scene. \n",
    "\n",
    "** Latent ** \n",
    " - size \n",
    " - neighborhood \n",
    " \n",
    "question: How best to condense our 4 features to 2, so that we really get to the heart of the information? \n",
    "\n",
    "What's teh most suitable feature selection tool? \n",
    "\n",
    "* SelectKBest \n",
    "* SelectPercentile\n",
    "\n",
    "You want to choose your $k$ number of features you would like to keep.\n",
    "\n",
    "** Crucial Ingredients ** \n",
    "- Many features, but I hypothesize a smaller no. of featurse actually driving the patterns \n",
    "- Try making a **composite feature** that more directly probes the underlying phenomednon\n",
    "\n",
    "## Example: Square Footage + No. Rooms -> Size \n",
    "\n",
    "<img src= \"p_images/example_2.png\" alt= \"PCA\" style=\"width: 500px;\"/> \n",
    "\n",
    "Essentially, what is occuring in this image is a projection of that vector by creating new orthognal vectors along the line, creating a transformation of new points along the vector.\n",
    "\n",
    "### How to determine the principal component. \n",
    "\n",
    "** Variance ** - the willingness / felxibility of an algorithm to learn (Machine Learning) \n",
    "\n",
    "** Variance (Stats) ** - roughly the \"spread\" of a data distribution $STD^{2}$. \n",
    "\n",
    "<img src= \"p_images/example_3.png\" alt= \"PCA\" style=\"width: 500px;\"/> \n",
    "\n",
    "The longer line is goign to be the direction of maximum variance. ** Principal Component** of the data set is the direction that has thed ** Largest Variance ** because it retains maximum amount of \"information\" in the orginal data.\n",
    "\n",
    "### Maximal Variance and information loss \n",
    "\n",
    "<img src= \"p_images/example_4.png\" alt= \"PCA\" style = \"widht: 500px;\"/> \n",
    "essentially, we want to creat a vector where the distance of the projection onto direction of maximal variance minimizes the distance from old (higher-dimensional) data point to its new transformed value. This in turn minimizes information loss. \n",
    "\n",
    "\n",
    "### PCA as a General Algorithm for feature transformation \n",
    "\n",
    "\n",
    "<img src= \"p_images/example_5.png\" alt= \"PCA\" style = \"widht: 500px;\"/> \n",
    "\n",
    "### Review / Definition of PCA \n",
    "\n",
    "- Systematized way to transform input features into principal components \n",
    "- use principal components as new features \n",
    "- PCs are directions in the data that maximize variance (minimize information loss) when you project / compress down onto them \n",
    "- more variance of data along a PC, higher than PC is ranked \n",
    "- most variance / most information --> first PC \n",
    "    - most variance (without overlapping w/ first PC) -- > Second PC\n",
    "- max no. of PCs = no. of input features. \n",
    "\n",
    "### PCA in SKLEARN \n",
    "\n",
    "``` python\n",
    "    def doPCA(): \n",
    "        from sklearn. decomposition import PCA \n",
    "        pca = PCA(n_components = 2) \n",
    "        pca.fit(data) \n",
    "        return pca \n",
    "    pca = doPCAA() # here it is folks. \n",
    "    print pca.explained_variance_ratio # explains its maximal variance eigenvalues \n",
    "    first_pc = pca.components_[0] # one component projection \n",
    "    second_pc = pca.components_[1] # second  . . . \n",
    "    \n",
    "    transform_data = pca.transform(data) \n",
    "    for ii, jj in zip(transformed_data, data): \n",
    "        \"\"\"This loop shows us what the transformed data is doing compared \n",
    "        to the original data set \"\"\"\n",
    "        \n",
    "        plt.scatter(first_pc[0]*ii[0], first_pc[1]*ii[0], color = \"r\") \n",
    "        plt.scatter(second_pc[0]*ii[0], second_pc[1]*ii[0], color = \"c\") \n",
    "        plot.scatter(jj[0], jj[1], color = \"b\") \n",
    "    \n",
    "```\n",
    "\n",
    "<img src=\"p_images/example_6.png\" alt = \"PCA\" style= \"width: 500px;\" /> \n",
    "As you can see from above we have some orthognality within the two vectors. \n",
    "\n",
    "\n",
    "### When to use PCA \n",
    "\n",
    "- Latent features driving the patterns in the data (big shots @ enron) \n",
    "- dimensionality reduction \n",
    "    - visualize high -dimensional data dim(3,4, . . ) proj down to 2 dims\n",
    "    - reduce noise \n",
    "    - make other algorithms (regression, classification) \n",
    "      - word better b/c fewer inputs (eigenfaces) \n",
    "\n",
    "\n",
    "### PCA for Facial Recongnition \n",
    "\n",
    "** What makes facial recognition in pictures good for PCA**? \n",
    "- pictures of faces generally have high input dimensions / many pixels \n",
    "- faces have general patterns that could be captured in smaller number of dimensions \n",
    "\n",
    "\n",
    "#### EigenFaces Code \n",
    "\n",
    "``` eigenfaces.py ``` in src code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
