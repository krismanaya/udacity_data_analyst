{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CYOA - Choose Your Own Adventure\n",
    "\n",
    "- choose your own algorithm to investigate. \n",
    "\n",
    "Algorithms (pick one surpervised classification algor\n",
    "\n",
    "* k nearest neighbors - classic, simple, easy to understand\n",
    "* adaboost - - > - > - > - > - > - >   - > - > - >   - > - > - >  \n",
    "* random forest -   \"ensemble methods\". Meta classifiers built from (usually) decision trees\n",
    "\n",
    "\n",
    "** Process ** \n",
    "\n",
    "1) Do some reasearch! get a general understand \n",
    "2) find sklearn documentation \n",
    "3) deploy it! try to get it running\n",
    "4) use it to make predictions \n",
    "5) evaluate it. What is the accuracy? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K nearest Neighbors Alogrithm \n",
    "\n",
    "**Definition** - Suppose we have pairs $(X,Y),(X_{1},Y_{1}),\\cdots,(X_{n},Y_{n}) \\in \\mathbb{R}^{d} \\times \\{0,1\\}$, where $Y$ is the class label of $X$, so that $X|Y = r \\sim P_{r}$ for $r = 1,2$(and probablity distributions $P_{r}$). GIven some norm $ \\|\\cdot\\|$ on $\\mathbb{R}^{d}$ and a point in $x \\in \\mathbb{R}^{d}$, let $(X,Y),(X_{(1)},Y_{(1)}),\\cdots,(X_{(n)},Y_{(n)})$ be a re ordering of the raining data such that  $ \\|X_{(1)} - x\\| \\leq \\cdots \\leq \\|X_{(n)} - x \\|$. \n",
    "\n",
    "### Example 1:\n",
    "\n",
    "<img src=\"cyo_images/example_1.png\\\" alt = \"knn\" style =\"width: 550px;\"/>\n",
    "\n",
    "\n",
    "Essentially, given this image above we have some point $x$ and we draw an $\\epsilon$ ball which is the smallest covering around the points we wish to classify. We choose a $k = n$ when we which to look at objects that are simmply assigned to teh class of that nearest neighbor. Within the example in the solid closed ball, if $k = 3$ then our point in space would be assigned to the $1$ classification. If $k=5$ it will then look to the five data points and choose a ball which is shown open around all the classifications. It will then be decided by the given metric which is menkowski how to choose the classfication. \n",
    "\n",
    "### Minkowski Metric: \n",
    "\n",
    "The Minkowski distance of order ''p'' between two points\n",
    "\n",
    "$$X=(x_1,x_2,\\ldots,x_n)\\text{ and }Y=(y_1,y_2,\\ldots,y_n) \\in \\mathbb{R}^n$$\n",
    "\n",
    "is defined as: \n",
    "$$\\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p}$$ \n",
    "For $p\\geq 1$, the Minkowski distance is a [[Metric (mathematics)|metric]] as a result of the [[Minkowski inequality]].  When $p<1$, the distance between (0,0) and (1,1) is $2^{1/p}>2$, but the point (0,1) is at a distance 1 from both of these points.  Since this violates the [[triangle inequality]], for $p<1$ it is not a metric.\n",
    "\n",
    "\n",
    "### Example 2: \n",
    "\n",
    "* Calculate using distance measure like Euclidean\n",
    "\n",
    "$d(p,q) = \\sqrt{\\sum_{i=1}^{n}(q_{i} - p_{i})^{2}}$ where in this case $p = 2$  \n",
    "suppose we have test data with class $Y = \\{Bad,Good\\}$ and the data is \n",
    "Test-Data $\\rightarrow$ acid durability$=3$, and strength$=7$, class$=Y$.Calculate the distance and then rank it from the closest distance $d$. \n",
    "\n",
    "<img src=\"cyo_images/example_2.png\\\" alt = \"knn\" style =\"width: 550px;\"/>\n",
    "\n",
    "in this case:  \n",
    "* $T_{1} = rank_{3}$, \n",
    "* $T_{2} = rank_{4}$, \n",
    "* $T_{3} = rank_{3}$, \n",
    "* $T_{4} = rank_{2}$. \n",
    "\n",
    "Since $T_{3}$ is the shortest distance and falls in the class $Good$ it's going to be the pest point when $k = 1$. Similarly when $k=2$ we have $T_{3}$ and $T_{4}$ respectively. For $k = 3$ we have $T_{1}$,$T_{2}$ and $T_{3}$, but here $T_{1}$ has class $Bad$, but with probality $P$ we can have assumption $\\frac{2}{3}$ chance of landing on good for the distribution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "[0]\n",
      "[[ 0.66666667  0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "#Code for (KNN)\n",
    "\n",
    "X = [[0],[1],[2], [3]] #training features \n",
    "Y = [0,0,1,1] #training labels\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNNC\n",
    "neigh = KNNC(n_neighbors = 3) #classifier \n",
    "neigh = neigh.fit(X,Y) #training\n",
    "print(neigh)\n",
    "\n",
    "print(neigh.predict([[1.1]]))\n",
    "print(neigh.predict_proba([[0.9]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
