Enron Submission Free-Response Questions
A critical part of machine learning is making sense of your analysis process and communicating it to others. The questions below will help us understand your decision-making process and allow us to give feedback on your project. Please answer each question; your answers should be about 1-2 paragraphs per question. If you find yourself writing much more than that, take a step back and see if you can simplify your response!


When your evaluator looks at your responses, he or she will use a specific list of rubric items to assess your answers. Here is the link to that rubric: [Link] Each question has one or more specific rubric items associated with it, so before you submit an answer, take a look at that part of the rubric. If your response does not meet expectations for all rubric points, you will be asked to revise and resubmit your project. Make sure that your responses are detailed enough that the evaluator will be able to understand the steps you took and your thought processes as you went through the data analysis.
Once you’ve submitted your responses, your coach will take a look and may ask a few more focused follow-up questions on one or more of your answers. 

 
We can’t wait to see what you’ve put together for this project!


1) Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]

ANSWER: The goal of this project was to try an find the best precision and recall score for POI identifies. If I high score is achieved, we may have a fine tuned classifier for detecting fraud through various emails accounts. The way that POIs were achieved were generated by individuals who were indicted, reached a settlement or plea deal with the United States government. While there were many outliers like Kenneth Lay who were consider POIs, some outliers were removed because they did not represent the overall dataset that was corresponding with detection of True POIs. For instance, the amount “Total” was included within the total_payments feature, which is in fact the sum of all payments received by each unique individual who worked at Enron, so this was not necessary. Secondly, the item “Travel Agency in the Park” which was Kenneth Lay’s younger sister Sharon’s co-owned company which received $362,096 total payments. I believed this data set did not seem necessary because it did not qualify as a unique individual. 


2) What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]

ANSWER: When I decided to approach the DecisionTreeClassifier as something that could possibly achieve our .3 threshold for precision and recall score I wanted to understand which features could impact the scaling process. After I attempted to re-create financial features to do ratios of finances and emails, I performed the object feature_importances_ within my DecisionTreeClassifier. I discovered that two financial ratios that I created ended up becoming very importance new re-scaled features for my classifier to identify POIs. My feature importances were as followed: 

######################################### Original Features ##############################################
Original Features sorted by their score:
[(0.36, 'total_stock_value'),
 (0.18, 'long_term_incentive'),
 (0.15, 'from_poi_to_this_person'),
 (0.13, 'from_this_person_to_poi'),
 (0.06, 'from_messages'),
 (0.05, 'other'),
 (0.05, 'expenses'),
 (0.03, 'exercised_stock_options'),
 (0.0, 'total_payments'),
 (0.0, 'to_messages'),
 (0.0, 'salary'),
 (0.0, 'restricted_stock_deferred'),
 (0.0, 'restricted_stock'),
 (0.0, 'loan_advances'),
 (0.0, 'director_fees'),
 (0.0, 'deferred_income'),
 (0.0, 'deferral_payments'),
 (0.0, 'bonus')]


######################################### New Features ##############################################
Original Features sorted by their score:
[(0.29, 'to_poi_ratio'),
 (0.26, 'restricted_ratio'),
 (0.22, 'exercised_ratio'),
 (0.02, 'salary_bonus_ratio'),
 (0.0, 'total_sp_ratio')]


######################################### Best Features ##############################################
Original Features sorted by their score:
[(0.47, 'other'),
 (0.22, 'total_stock_value'),
 (0.08, 'from_this_person_to_poi'),
 (0.07, 'long_term_incentive'),
 (0.05, 'to_poi_ratio'),
 (0.05, 'restricted_ratio'),
 (0.03, 'expenses'),
 (0.03, 'exercised_stock_options'),
 (0.0, 'from_poi_to_this_person'),
 (0.0, 'from_messages'),
 (0.0, 'exercised_ratio')]


While deciding the the best F1 score we have also seen above what features are important in obtaining this score. I believe all the other features are just going to generalize too much and are not useful for chosing this classifier. The best score I could obtain was 0.483, which is a little off of the .5 mark I wanted to settle with. What we see above is the outlined features, one with the new created features, old features. When intersected the fetures, I got the Best Features let's see how the run time is for this using GridSearch using best featuers

3) What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]

ANSWER: I picked DecisionTreeClassifier. Here is my analysis of the Classifiers. If you want to see all the metrics that were performed as well please visit my jupyter notebook. 

With the first pass with all my algorithms I've decided is actually not bad. Each algorithms had no tuning within the parameters and 2 out of 4 were able to detect some sort of precision with POIs. However, it looks like the Support Vector Machine did not produce any result along the POIs so it doesn't look like a good model for predicting fraud. Since this is binary operation, I was pretty sure the Decision Tree or K-Nearest Neighbors would perform well, but I did not realize the Naive Bayes would perform as great as it is performing. I believe the problem with K-Nearest neighbors is that it needs to have some tuning parameters and a better distance relation to represent some sort of precision or f1-score. Let's try and find the best parameters for these algorithms. 


* Classifiers Choices 
    * GaussianNB - NB looks pretty good with a thresh hold of .44 within the F1 score. However, since there aren't any really choices of choosing parameters to tune the features provide it might be best to work on another Classifier. 
    
    * SVC - I'm going to rule out any help that SVC could provide us with, it seems the threshold is .00 and it would be a very long road it tune the parameters to get a quality F1 score. 
    
    * KNN - Like SVC, KNN threshold produces a .00 F1 score I will once again pass on this classifier because I personally believe this is not the right choice to perform binary operations. 
    
    * DTC - I believe this is a good choice. We have a thresh hold of .22 on the F1 with no parameters and I believe with the right tuning on the features we might be able to bring this up to .50. I think by understanding the feature selections and performing a GridSearch along the Classifier we might be able to fine tune the Classifier to achieve our goals. 



4) What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]

ANSWER: Well we want to optimize the best performance of our algorithm based on the data so the parameters give us that option
if certain parameters are not tuned well it can lead to overfitting or incorrectly outputting a correct accuracy this is simple known as (TUNING) the parameters. The main goal of tuning our DTC is  to create a well readable precision and recall score for Identify POIs within the data set. We may have a possibility of overfitting our model which it will learn the detailed noise to the extent that it impacts the performance rate of the new model. For example, it is possible that DTC could be subject to overfitting by fine tuning parameters. Underfitting is possible is well, but we remedy this by trying to tackler other learning algorithms. 




5) What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”]

ANSWER: What is validation - Model validation is the preferred training method of building a model. This is important because it is used for parameters selections and to avoid overfitting. A classic mistake would be if your model is non-linear and is training on a training set only, it could be likely to get 100% accuracy and overfit. This would cause poor performance on the test set. A way I avoided this within my analysis was to use cross validation methods. 

6) Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]

ANSWER: I used a classification report which gives us the precision, recall, accuracy, F1 and F2 scores. Below is my metrics for my first pass of the Decision Tree Classifier. If you’d like to view each classification report for each classifier please view the python notebook: 

DecisionTreeClassifier(class_weight='balanced', criterion='entropy',
            max_depth=2, max_features=None, max_leaf_nodes=None,
            min_samples_leaf=1, min_samples_split=0.99,
            min_weight_fraction_leaf=0.0, presort=False, random_state=42,
            splitter='best')
	Accuracy: 0.82521	Precision: 0.42741	Recall: 0.65800	F1: 0.51821	F2: 0.59392
	Total predictions: 14000	True positives: 1316	False positives: 1763	False negatives:  684	True negatives: 10237

With our precision and recall metric it is useful to recall a confusion matrix:

                      Condition: A             Not A

  Test says “A”       True positive (TP)   |   False positive (FP)
                      ----------------------------------
  Test says “Not A”   False negative (FN)  |    True negative (TN)


Recall is TP/(TP + FN) whereas precision is TP/(TP+FP). Within information retrieval we would like to identity as many relevant documents (POIs / NONPOIs). Secondly, we would like to have to avoid sorting through junk within the data set this is defined as the Precision. So essentially, a 43% precision and 66% recall is pretty okay. Meaning, within these metrics Given a positive example of a identifying a POI there is a 66% chance the classifier will detect it (RECALL) and Given a positive prediction from the classifier how likely is this going to be correct this is (PRECISION). 




