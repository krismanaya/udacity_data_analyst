Enron Submission Free-Response Questions
A critical part of machine learning is making sense of your analysis process and communicating it to others. The questions below will help us understand your decision-making process and allow us to give feedback on your project. Please answer each question; your answers should be about 1-2 paragraphs per question. If you find yourself writing much more than that, take a step back and see if you can simplify your response!


When your evaluator looks at your responses, he or she will use a specific list of rubric items to assess your answers. Here is the link to that rubric: [Link] Each question has one or more specific rubric items associated with it, so before you submit an answer, take a look at that part of the rubric. If your response does not meet expectations for all rubric points, you will be asked to revise and resubmit your project. Make sure that your responses are detailed enough that the evaluator will be able to understand the steps you took and your thought processes as you went through the data analysis.
Once you’ve submitted your responses, your coach will take a look and may ask a few more focused follow-up questions on one or more of your answers. 

 
We can’t wait to see what you’ve put together for this project!


1) Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]

Answer: The goal of this project was to try an find the best precision and recall score for POI identifies. If I high score is achieved, we may have a fine tuned classifier for detecting fraud through various emails accounts. The way that POIs were achieved were generated by individuals who were indicted, reached a settlement or plea deal with the United States government. While there were many outliers like Kenneth Lay who were consider POIs, some outliers were removed because they did not represent the overall dataset that was corresponding with detection of True POIs. For instance, the amount “Total” was included within the total_payments feature, which is in fact the sum of all payments received by each unique individual who worked at Enron, so this was not necessary. Secondly, the item “Travel Agency in the Park” which was Kenneth Lay’s younger sister Sharon’s co-owned company which received $362,096 total payments. I believed this data set did not seem necessary because it did not qualify as a unique individual. 


2) What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]

When I decided to approach the DecisionTreeClassifier as something that could possibly achieve our .3 threshold for precision and recall score I wanted to understand which features could impact the scaling process. After I attempted to re-create financial features to do ratios of finances and emails, I performed the object feature_importances_ within my DecisionTreeClassifier. I discovered that two financial ratios that I created ended up becoming very importance new re-scaled features for my classifier to identify POIs. My feature importances were as followed: 

Features sorted by their score:
[(0.36, 'total_stock_value'),
 (0.25, 'exercised_stock_options'),
 (0.19, 'restricted_ratio'),
 (0.13, 'from_this_person_to_poi'),
 (0.07, 'exercised_ratio'),
 (0.0, 'total_sp_ratio'),
 (0.0, 'total_payments'),
 (0.0, 'to_poi_ratio'),
 (0.0, 'to_messages'),
 (0.0, 'shared_receipt_with_poi'),
 (0.0, 'salary_bonus_ratio'),
 (0.0, 'salary'),
 (0.0, 'restricted_stock_deferred'),
 (0.0, 'restricted_stock'),
 (0.0, 'poi'),
 (0.0, 'other'),
 (0.0, 'long_term_incentive'),
 (0.0, 'loan_advances'),
 (0.0, 'from_poi_to_this_person'),
 (0.0, 'from_messages'),
 (0.0, 'expenses'),
 (0.0, 'director_fees'),
 (0.0, 'deferred_income'),
 (0.0, 'deferral_payments'),
 (0.0, 'bonus')]

In the end I actually selected all the features because it ended up performing the best (except email). My analysis for the new features can be view within the Python Notebook under feature selection. 

3) What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]

I picked DecisionTreeClassifier. Here is my analysis of the Classifiers. If you want to see all the metrics that were performed as well please visit my jupyter notebook. 

With the first pass with all my algorithms I've decided is actually not bad. Each algorithms had no tuning within the parameters and 2 out of 4 were able to detect some sort of precision with POIs. However, it looks like the Support Vector Machine did not produce any result along the POIs so it doesn't look like a good model for predicting fraud. Since this is binary operation, I was pretty sure the Decision Tree or K-Nearest Neighbors would perform well, but I did not realize the Naive Bayes would perform as great as it is performing. I believe the problem with K-Nearest neighbors is that it needs to have some tuning parameters and a better distance relation to represent some sort of precision or f1-score. Let's try and find the best parameters for these algorithms. 


* Classifiers Choices 
    * GaussianNB - NB looks pretty good with a thresh hold of .44 within the F1 score. However, since there aren't any really choices of choosing parameters to tune the features provide it might be best to work on another Classifier. 
    
    * SVC - I'm going to rule out any help that SVC could provide us with, it seems the threshold is .00 and it would be a very long road it tune the parameters to get a quality F1 score. 
    
    * KNN - Like SVC, KNN threshold produces a .00 F1 score I will once again pass on this classifier because I personally believe this is not the right choice to perform binary operations. 
    
    * DTC - I believe this is a good choice. We have a thresh hold of .22 on the F1 with no parameters and I believe with the right tuning on the features we might be able to bring this up to .50. I think by understanding the feature selections and performing a GridSearch along the Classifier we might be able to fine tune the Classifier to achieve our goals. 



4) What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]

Well we want to optimize the best performance of our algorithm based on the data so the parameters give us that option
if certain parameters are not tuned well it can lead to overfitting or incorrectly outputting a correct accuracy. Having said that, the primary goal for this project was to set our parameters to optimize the results of POIs. I tuned my DecisionTreeClassifier parameters using grid search were I selected the best random_state, criterion, class_weight, depth_range, leaf_range, sample_split and let GridSearch run each test and output the best score based on the parameters I asked it to model. This can be quite lengthy in fact it took roughly 184.325 seconds to fine tune these parameters to my liking

5) What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”]

Essentially, I followed the task that was asked of me in the training videos. I started a model construction, selection of architecture and training parameters for my classifier, model training using the data set that was provided, model the validation by applying cross validation or least-squares, when back and went through the process again until i was satisfied then I selected an optimal best model and finally modeled testing using the entire independent data set.


6) Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]

I used a classification report which gives us the precision, recall, accuracy, F1 and F2 scores. Below is my metrics for my first pass of the Decision Tree Classifier. If you’d like to view each classification report for each classifier please view the python notebook: 


            precision    recall  f1-score   support

        0.0       0.89      0.91      0.90        34
        1.0       0.25      0.20      0.22         5

avg / total       0.80      0.82      0.81        39
('training time:', 0.003, 's')

Right of the bat you see that average time looks pretty good within the 80% range is something that I strive for in life. I will say that it’s detection model for identifying Non-POIs is higher this primary has to do with the fact that there is basically more Non-POIs for the computer to choose from then POIs. Essentially what this would report is with some precision an 89% chance of detecting a Non-POI within the data set. However, the Precision for Detecting a POIs within the Dataset is only 25%, this is why we needed to fine-tune our parameters se we could get a better scoring. 


