{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "## Homework problems for DataWrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is how you manually parse a cvs file in a dictionary.\n",
    "# Your task is to read the input DATAFILE line by line, and for the first 10 lines (not including the header)\n",
    "# split each line on \",\" and then for each line, create a dictionary\n",
    "# where the key is the header title of the field, and the value is the value of that field in the row.\n",
    "# The function parse_file should return a list of dictionaries,\n",
    "# each data line in the file being a single list entry.\n",
    "# Field names and values should not contain extra whitespace, like spaces or newline characters.\n",
    "# You can use the Python string method strip() to remove the extra whitespace.\n",
    "# You have to parse only the first 10 data lines in this exercise,\n",
    "# so the returned list should have 10 entries!\n",
    "\n",
    "def parse_file(datafile): \n",
    "    data = []\n",
    "    with open(datafile,'rb') as f: \n",
    "        header = f.readline().split(',')\n",
    "        counter = 0\n",
    "        for line in f: \n",
    "            if counter == 10: \n",
    "                break \n",
    "            d = {}\n",
    "            fields = line.split(',') \n",
    "            for i,value in enumerate(fields): \n",
    "                d[header[i].strip()] = value.strip()\n",
    "            data.append(d)\n",
    "            counter += 1\n",
    "    return data\n",
    "\n",
    "#Parse File w/o csv module\n",
    "# parse_file('beatles-diskography.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'BPI Certification': 'Gold',\n",
       "  'Label': 'Parlophone(UK)',\n",
       "  'RIAA Certification': 'Platinum',\n",
       "  'Released': '22 March 1963',\n",
       "  'Title': 'Please Please Me',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '\\xe2\\x80\\x94'},\n",
       " {'BPI Certification': 'Platinum',\n",
       "  'Label': 'Parlophone(UK)',\n",
       "  'RIAA Certification': 'Gold',\n",
       "  'Released': '22 November 1963',\n",
       "  'Title': 'With the Beatles',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '\\xe2\\x80\\x94'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(CAN)',\n",
       "  'RIAA Certification': '',\n",
       "  'Released': '25 November 1963',\n",
       "  'Title': 'Beatlemania! With the Beatles',\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '\\xe2\\x80\\x94'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Vee-Jay(US)',\n",
       "  'RIAA Certification': '',\n",
       "  'Released': '10 January 1964',\n",
       "  'Title': 'Introducing... The Beatles',\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '2'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(US)',\n",
       "  'RIAA Certification': '5xPlatinum',\n",
       "  'Released': '20 January 1964',\n",
       "  'Title': 'Meet the Beatles!',\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(CAN)',\n",
       "  'RIAA Certification': '',\n",
       "  'Released': '3 February 1964',\n",
       "  'Title': 'Twist and Shout',\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '\\xe2\\x80\\x94'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(US)',\n",
       "  'RIAA Certification': '2xPlatinum',\n",
       "  'Released': '10 April 1964',\n",
       "  'Title': \"The Beatles' Second Album\",\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(CAN)',\n",
       "  'RIAA Certification': '',\n",
       "  'Released': '11 May 1964',\n",
       "  'Title': \"The Beatles' Long Tall Sally\",\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '\\xe2\\x80\\x94'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'United Artists(US)[C]',\n",
       "  'RIAA Certification': '4xPlatinum',\n",
       "  'Released': '26 June 1964',\n",
       "  'Title': \"A Hard Day's Night\",\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': 'Gold',\n",
       "  'Label': 'Parlophone(UK)',\n",
       "  'RIAA Certification': '',\n",
       "  'Released': '10 July 1964',\n",
       "  'Title': '',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '\\xe2\\x80\\x94'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(US)',\n",
       "  'RIAA Certification': 'Platinum',\n",
       "  'Released': '20 July 1964',\n",
       "  'Title': 'Something New',\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '2'},\n",
       " {'BPI Certification': 'Gold',\n",
       "  'Label': 'Parlophone(UK)',\n",
       "  'RIAA Certification': 'Platinum',\n",
       "  'Released': '4 December 1964',\n",
       "  'Title': 'Beatles for Sale',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '\\xe2\\x80\\x94'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(US)',\n",
       "  'RIAA Certification': '3xPlatinum',\n",
       "  'Released': '15 December 1964',\n",
       "  'Title': \"Beatles '65\",\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Parlophone(NZ), Capitol(US)',\n",
       "  'RIAA Certification': 'Platinum',\n",
       "  'Released': '14 June 1965',\n",
       "  'Title': 'Beatles VI',\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': 'Platinum',\n",
       "  'Label': 'Parlophone(UK)',\n",
       "  'RIAA Certification': '',\n",
       "  'Released': '6 August 1965',\n",
       "  'Title': 'Help!',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '\\xe2\\x80\\x94'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(US)[C]',\n",
       "  'RIAA Certification': '3xPlatinum',\n",
       "  'Released': '13 August 1965',\n",
       "  'Title': '',\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': 'Platinum',\n",
       "  'Label': 'Parlophone(UK)',\n",
       "  'RIAA Certification': '',\n",
       "  'Released': '3 December 1965',\n",
       "  'Title': 'Rubber Soul',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '\\xe2\\x80\\x94'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(US)[C]',\n",
       "  'RIAA Certification': '6xPlatinum',\n",
       "  'Released': '6 December 1965',\n",
       "  'Title': '',\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(US)',\n",
       "  'RIAA Certification': '2xPlatinum',\n",
       "  'Released': '15 June 1966',\n",
       "  'Title': 'Yesterday and Today',\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': 'Platinum',\n",
       "  'Label': 'Parlophone(UK)',\n",
       "  'RIAA Certification': '',\n",
       "  'Released': '5 August 1966',\n",
       "  'Title': 'Revolver',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '\\xe2\\x80\\x94'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(US)[C]',\n",
       "  'RIAA Certification': '5xPlatinum',\n",
       "  'Released': '8 August 1966',\n",
       "  'Title': '',\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': '3xPlatinum',\n",
       "  'Label': 'Parlophone(UK), Capitol(US)',\n",
       "  'RIAA Certification': '11xPlatinum',\n",
       "  'Released': '1 June 1967',\n",
       "  'Title': \"Sgt. Pepper's Lonely Hearts Club Band\",\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': 'Platinum',\n",
       "  'Label': 'Parlophone(UK), Capitol(US)',\n",
       "  'RIAA Certification': '6xPlatinum',\n",
       "  'Released': '27 November 1967',\n",
       "  'Title': 'Magical Mystery Tour',\n",
       "  'UK Chart Position': '31[D]',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': 'Platinum',\n",
       "  'Label': 'Apple(UK), Capitol(US)',\n",
       "  'RIAA Certification': '19xPlatinum',\n",
       "  'Released': '22 November 1968',\n",
       "  'Title': 'The Beatles',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': 'Silver',\n",
       "  'Label': 'Apple(UK), Capitol(US)',\n",
       "  'RIAA Certification': 'Platinum',\n",
       "  'Released': '13 January 1969',\n",
       "  'Title': 'Yellow Submarine',\n",
       "  'UK Chart Position': '3',\n",
       "  'US Chart Position': '2'},\n",
       " {'BPI Certification': '2xPlatinum',\n",
       "  'Label': 'Apple(UK), Capitol(US)',\n",
       "  'RIAA Certification': '12xPlatinum',\n",
       "  'Released': '26 September 1969',\n",
       "  'Title': 'Abbey Road',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': 'Gold',\n",
       "  'Label': 'Apple(UK),United Artists(US)',\n",
       "  'RIAA Certification': '4xPlatinum',\n",
       "  'Released': '8 May 1970',\n",
       "  'Title': 'Let It Be',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '1'}]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now parse the data using DictReader csv module\n",
    "import csv\n",
    "def parse_csv_dr(datafile): \n",
    "    data = []\n",
    "    n = 0 \n",
    "    with open(datafile,'rb') as sd: \n",
    "        r = csv.DictReader(sd)\n",
    "        for line in r: \n",
    "            data.append(line)\n",
    "    return data\n",
    "\n",
    "parse_csv_dr('beatles-diskography.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Intro to the module XLRD for Excel files. \n",
    "import xlrd \n",
    "\n",
    "#import the file\n",
    "datafile = '2013_ERCOT_Hourly_Load_Data.xls'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List Comprehension\n",
      "data[3][2]: 1036.2167394\n",
      "\n",
      "Cells in a nested loop:\n",
      "41277.0833333 9240.30996952 1438.44928594 1565.62980305 894.858053634 14013.232226 3028.50276713 6165.28163968 1157.91411201 37504.177857 \n",
      "ROWS, COLUMNS and CELLS:\n",
      "Number of rows in the sheet: 8761\n",
      "Type of data in cell (row 3, col 2): 2\n",
      "Value in cell (row 3, col 2): 1036.2167394\n",
      "Get a slice of values in column 3, from rows 1-3:\n",
      "[1411.823158897756, 1403.6010926983663, 1395.1312126407752]\n",
      "\n",
      "DATES:\n",
      "Tytpe of data in cell (row 1, col 0): 3\n",
      "Time in Excel format: 41275.0416667\n",
      "Convert time to a Python datetime tuple, from the Excel float: (2013, 1, 1, 1, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# create a parse_file function \n",
    "def parse_file(datafile): \n",
    "    workbook = xlrd.open_workbook(datafile) # open the excel file \n",
    "    sheet = workbook.sheet_by_index(0) # indicate what sheet we use \n",
    "    \n",
    "    data = [[sheet.cell_value(r,col)\n",
    "                for col in range(sheet.ncols)]\n",
    "                   for r in range(sheet.nrows)] # iterating through the sheet into a list \n",
    "    \n",
    "    print \"\\nList Comprehension\"\n",
    "    print \"data[3][2]:\", \n",
    "    print data[3][2]\n",
    "    \n",
    "    print \"\\nCells in a nested loop:\"\n",
    "    for row in range(sheet.nrows): \n",
    "        for col in range(sheet.ncols): \n",
    "            if row == 50: \n",
    "                print sheet.cell_value(row, col), \n",
    "                \n",
    "             \n",
    "    ### other useful methods: \n",
    "    print \"\\nROWS, COLUMNS and CELLS:\"\n",
    "    print \"Number of rows in the sheet:\",\n",
    "    print sheet.nrows\n",
    "    print \"Type of data in cell (row 3, col 2):\", \n",
    "    print sheet.cell_type(3,2)\n",
    "    print \"Value in cell (row 3, col 2):\", \n",
    "    print sheet.cell_value(3,2)\n",
    "    print \"Get a slice of values in column 3, from rows 1-3:\"\n",
    "    print sheet.col_values(3,start_rowx=1,end_rowx=4)\n",
    "    \n",
    "    print \"\\nDATES:\"\n",
    "    print \"Tytpe of data in cell (row 1, col 0):\", \n",
    "    print sheet.cell_type(1,0)\n",
    "    exceltime = sheet.cell_value(1,0)\n",
    "    print \"Time in Excel format:\", \n",
    "    print exceltime\n",
    "    print \"Convert time to a Python datetime tuple, from the Excel float:\",\n",
    "    print xlrd.xldate_as_tuple(exceltime, 0)\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = parse_file(datafile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting http://musicbrainz.org/ws/2/artist/?query=artist%3ANirvana&fmt=json\n",
      "{\n",
      "    \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
      "    \"name\": \"United Kingdom\", \n",
      "    \"sort-name\": \"United Kingdom\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# To experiment with this code freely you will have to run this code locally.\n",
    "# Take a look at the main() function for an example of how to use the code.\n",
    "# We have provided example json output in the other code editor tabs for you to\n",
    "# look at, but you will not be able to run any queries through our UI.\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "BASE_URL = \"http://musicbrainz.org/ws/2/\"\n",
    "ARTIST_URL = BASE_URL + \"artist/\"\n",
    "\n",
    "# query parameters are given to the requests.get function as a dictionary; this\n",
    "# variable contains some starter parameters.\n",
    "query_type = {  \"simple\": {},\n",
    "                \"atr\": {\"inc\": \"aliases+tags+ratings\"},\n",
    "                \"aliases\": {\"inc\": \"aliases\"},\n",
    "                \"releases\": {\"inc\": \"releases\"}}\n",
    "\n",
    "\n",
    "def query_site(url, params, uid=\"\", fmt=\"json\"):\n",
    "    # This is the main function for making queries to the musicbrainz API.\n",
    "    # A json document should be returned by the query.\n",
    "    params[\"fmt\"] = fmt\n",
    "    r = requests.get(url + uid, params=params)\n",
    "    print \"requesting\", r.url\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def query_by_name(url, params, name):\n",
    "    # This adds an artist name to the query parameters before making\n",
    "    # an API call to the function above.\n",
    "    params[\"query\"] = \"artist:\" + name\n",
    "    return query_site(url, params)\n",
    "\n",
    "\n",
    "def pretty_print(data, indent=4):\n",
    "    # After we get our output, we can format it to be more readable\n",
    "    # by using this function.\n",
    "    if type(data) == dict:\n",
    "        print json.dumps(data, indent=indent, sort_keys=True)\n",
    "    else:\n",
    "        print data\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    Modify the function calls and indexing below to answer the questions on\n",
    "    the next quiz. HINT: Note how the output we get from the site is a\n",
    "    multi-level JSON document, so try making print statements to step through\n",
    "    the structure one level at a time or copy the output to a separate output\n",
    "    file.\n",
    "    '''\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"Nirvana\")\n",
    "    pretty_print(results['artists'][0]['area'])\n",
    "\n",
    "#     artist_id = results[\"artists\"][0]['id']\n",
    "#     print \"\\nARTIST:\"\n",
    "#     pretty_print(results[\"artists\"][0])\n",
    "\n",
    "#     artist_data = query_site(ARTIST_URL, query_type[\"releases\"], artist_id)\n",
    "#     pretty_print(artist_data) \n",
    "#     releases = artist_data[\"releases\"]\n",
    "#     print \"\\nONE RELEASE:\"\n",
    "#     pretty_print(releases[0], indent=2)\n",
    "#     release_titles = [r[\"title\"] for r in releases]\n",
    "\n",
    "#     print \"\\nALL TITLES:\"\n",
    "#     for t in release_titles:\n",
    "#         print t\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Children of root:\n",
      "ui\n",
      "ji\n",
      "fm\n",
      "bdy\n",
      "bm\n"
     ]
    }
   ],
   "source": [
    "## XML training. \n",
    "import xml.etree.ElementTree as ET \n",
    "import pprint \n",
    "\n",
    "# XML: parse the file \n",
    "tree = ET.parse('exampleResearchArticle.xml')\n",
    "\n",
    "# Get its root \n",
    "root = tree.getroot() \n",
    "\n",
    "# print the object \n",
    "print \"\\nChildren of root:\"\n",
    "for child in root: \n",
    "    print child.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title:\n",
      "Standardization of the functional syndesmosis widening by dynamic U.S examination\n"
     ]
    }
   ],
   "source": [
    "# extract from fm to find its children and find its title\n",
    "title = root.find(\"./fm/bibl/title\")\n",
    "title_text = \"\"\n",
    "for p in title: \n",
    "    title_text += p.text\n",
    "print \"\\nTitle:\\n\", title_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Author email addresses:\n",
      "omer@extremegate.com\n",
      "mcarmont@hotmail.com\n",
      "laver17@gmail.com\n",
      "nyska@internet-zahav.net\n",
      "kammarh@gmail.com\n",
      "gideon.mann.md@gmail.com\n",
      "barns.nz@gmail.com\n",
      "eukots@gmail.com\n"
     ]
    }
   ],
   "source": [
    "# find the author email addresses\n",
    "print \"\\nAuthor email addresses:\"\n",
    "for a in root.findall(\"./fm/bibl/aug/au\"): \n",
    "    email = a.find('email')\n",
    "    if email is not None: \n",
    "        print email.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# authors = []\n",
    "# for author in root.findall(\"./fm/bibl/aug/au\"): \n",
    "#     data = {'fnm': None, \"snm\": None, \"email\": None, 'insr': []}\n",
    "#     data['fnm'] = author.find('./fnm').text\n",
    "#     data['snm'] = author.find('./snm').text\n",
    "#     data['email'] = author.find('./email').text\n",
    "#     insr = author.findall(\"./insr\")\n",
    "#     for i in insr: \n",
    "#         data['insr'].append(i.attrib['iid'])\n",
    "#     auother.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carriers:\n",
      "All\n",
      "AllUS\n",
      "AllForeign\n",
      "AS\n",
      "G4\n",
      "AA\n",
      "5Y\n",
      "DL\n",
      "MQ\n",
      "EV\n",
      "F9\n",
      "HA\n",
      "B6\n",
      "OO\n",
      "WN\n",
      "NK\n",
      "UA\n",
      "VX\n",
      "\n",
      "Airports:\n",
      "All\n",
      "AllMajors\n",
      "ATL\n",
      "BWI\n",
      "BOS\n",
      "CLT\n",
      "MDW\n",
      "ORD\n",
      "DAL\n",
      "DFW\n",
      "DEN\n",
      "DTW\n",
      "FLL\n",
      "IAH\n",
      "LAS\n",
      "LAX\n",
      "MIA\n",
      "MSP\n",
      "JFK\n",
      "LGA\n",
      "EWR\n",
      "MCO\n",
      "PHL\n",
      "PHX\n",
      "PDX\n",
      "SLC\n",
      "SAN\n",
      "SFO\n",
      "SEA\n",
      "TPA\n",
      "DCA\n",
      "IAD\n",
      "AllOthers\n",
      "UXM\n",
      "ABR\n",
      "ABI\n",
      "DYS\n",
      "ADK\n",
      "VZF\n",
      "BQN\n",
      "AKK\n",
      "KKI\n",
      "AKI\n",
      "AKO\n",
      "CAK\n",
      "7AK\n",
      "KQA\n",
      "AUK\n",
      "ALM\n",
      "ALS\n",
      "ABY\n",
      "ALB\n",
      "ABQ\n",
      "ZXB\n",
      "WKK\n",
      "AED\n",
      "AEX\n",
      "AXN\n",
      "AET\n",
      "ABE\n",
      "AIA\n",
      "APN\n",
      "DQH\n",
      "AOO\n",
      "AMA\n",
      "ABL\n",
      "OQZ\n",
      "AOS\n",
      "OTS\n",
      "AKP\n",
      "EDF\n",
      "DQL\n",
      "MRI\n",
      "ANC\n",
      "AND\n",
      "AGN\n",
      "ANI\n",
      "ANN\n",
      "ANB\n",
      "ANV\n",
      "ATW\n",
      "ACV\n",
      "ARC\n",
      "ADM\n",
      "AVL\n",
      "HTS\n",
      "ASE\n",
      "AST\n",
      "AHN\n",
      "AKB\n",
      "PDK\n",
      "FTY\n",
      "ACY\n",
      "ATT\n",
      "ATK\n",
      "MER\n",
      "AUO\n",
      "AGS\n",
      "AUG\n",
      "AUS\n",
      "A28\n",
      "BFL\n",
      "BGR\n",
      "BHB\n",
      "BRW\n",
      "BTI\n",
      "BQV\n",
      "A2K\n",
      "BTR\n",
      "BTL\n",
      "AK2\n",
      "A56\n",
      "BTY\n",
      "BPT\n",
      "BVD\n",
      "WBQ\n",
      "BKW\n",
      "BED\n",
      "A11\n",
      "KBE\n",
      "BLV\n",
      "BLI\n",
      "BLM\n",
      "JVL\n",
      "BVU\n",
      "BJI\n",
      "RDM\n",
      "BEH\n",
      "BET\n",
      "BTT\n",
      "BVY\n",
      "OQB\n",
      "A50\n",
      "BIC\n",
      "BIG\n",
      "BGQ\n",
      "BMX\n",
      "PWR\n",
      "A85\n",
      "BIL\n",
      "BIX\n",
      "BGM\n",
      "KBC\n",
      "BHM\n",
      "BIS\n",
      "BYW\n",
      "BID\n",
      "BMG\n",
      "BMI\n",
      "BFB\n",
      "BYH\n",
      "BCT\n",
      "BOI\n",
      "RLU\n",
      "BXS\n",
      "BLD\n",
      "BYA\n",
      "BWG\n",
      "BZN\n",
      "BFD\n",
      "A23\n",
      "BRD\n",
      "BKG\n",
      "PWT\n",
      "KTS\n",
      "BDR\n",
      "TRI\n",
      "BKX\n",
      "RBH\n",
      "BRO\n",
      "BWD\n",
      "BQK\n",
      "BCE\n",
      "BKC\n",
      "BUF\n",
      "IFP\n",
      "BUR\n",
      "BRL\n",
      "BTV\n",
      "MVW\n",
      "BNO\n",
      "BTM\n",
      "JQF\n",
      "UXI\n",
      "CDW\n",
      "C01\n",
      "ADW\n",
      "CDL\n",
      "CGI\n",
      "LUR\n",
      "EHM\n",
      "CZF\n",
      "A61\n",
      "A40\n",
      "CYT\n",
      "MDH\n",
      "CLD\n",
      "CNM\n",
      "A87\n",
      "CPR\n",
      "CDC\n",
      "CID\n",
      "JRV\n",
      "NRR\n",
      "CEM\n",
      "CDR\n",
      "CIK\n",
      "CMI\n",
      "WCR\n",
      "CHS\n",
      "CRW\n",
      "SPB\n",
      "STT\n",
      "CHO\n",
      "CYM\n",
      "CHA\n",
      "CYF\n",
      "WA7\n",
      "CEX\n",
      "EGA\n",
      "NCN\n",
      "KCN\n",
      "VAK\n",
      "CYS\n",
      "PWK\n",
      "CHI\n",
      "DPA\n",
      "LOT\n",
      "CKX\n",
      "CIC\n",
      "CEF\n",
      "KCG\n",
      "KCL\n",
      "WQZ\n",
      "KCQ\n",
      "CZN\n",
      "CIV\n",
      "ZXH\n",
      "SSB\n",
      "STX\n",
      "CHU\n",
      "LUK\n",
      "CVG\n",
      "OQC\n",
      "A12\n",
      "CHP\n",
      "IRC\n",
      "CLP\n",
      "CKB\n",
      "BKL\n",
      "CLE\n",
      "CGF\n",
      "CFT\n",
      "CLK\n",
      "ZXN\n",
      "CVN\n",
      "ZXI\n",
      "OOB\n",
      "COD\n",
      "CFA\n",
      "KCC\n",
      "A69\n",
      "CDB\n",
      "CXF\n",
      "CLL\n",
      "KCR\n",
      "COS\n",
      "COA\n",
      "COU\n",
      "CAE\n",
      "CSG\n",
      "CBM\n",
      "GTR\n",
      "OSU\n",
      "CMH\n",
      "LCK\n",
      "CCR\n",
      "CKU\n",
      "CDV\n",
      "CBA\n",
      "CRP\n",
      "CEZ\n",
      "CVO\n",
      "CIL\n",
      "CGA\n",
      "CEC\n",
      "CKD\n",
      "CUW\n",
      "CPX\n",
      "CBE\n",
      "DCK\n",
      "ADS\n",
      "RBD\n",
      "AFW\n",
      "FTW\n",
      "DGB\n",
      "DAN\n",
      "WQW\n",
      "MGY\n",
      "DAY\n",
      "DAB\n",
      "AA8\n",
      "SCC\n",
      "FVZ\n",
      "A02\n",
      "DTH\n",
      "DTR\n",
      "DEC\n",
      "XXV\n",
      "A36\n",
      "DHB\n",
      "DRG\n",
      "DRT\n",
      "DLF\n",
      "DJN\n",
      "DMN\n",
      "DTO\n",
      "APA\n",
      "FTG\n",
      "DSM\n",
      "DSI\n",
      "DTL\n",
      "DET\n",
      "DTT\n",
      "YIP\n",
      "DVL\n",
      "DIK\n",
      "DLG\n",
      "DIO\n",
      "DDC\n",
      "FVQ\n",
      "DOF\n",
      "DHN\n",
      "DOV\n",
      "DRF\n",
      "A22\n",
      "FQQ\n",
      "DUJ\n",
      "DBQ\n",
      "DLH\n",
      "A4K\n",
      "AMK\n",
      "DRO\n",
      "EAA\n",
      "EGE\n",
      "FRG\n",
      "HTO\n",
      "GA0\n",
      "ESN\n",
      "ESD\n",
      "EAU\n",
      "EDA\n",
      "EDW\n",
      "EEK\n",
      "EGX\n",
      "KKU\n",
      "KEK\n",
      "ZXO\n",
      "IPL\n",
      "ELD\n",
      "BIF\n",
      "ELP\n",
      "ELV\n",
      "ELI\n",
      "EKO\n",
      "ELM\n",
      "LYU\n",
      "ELY\n",
      "EMK\n",
      "WDG\n",
      "ERI\n",
      "ESC\n",
      "EUG\n",
      "EVV\n",
      "EVM\n",
      "PAE\n",
      "EXI\n",
      "EIL\n",
      "FAI\n",
      "FBK\n",
      "A01\n",
      "A6K\n",
      "SUU\n",
      "FAJ\n",
      "KFP\n",
      "FWL\n",
      "FAR\n",
      "FMN\n",
      "FYV\n",
      "XNA\n",
      "FAY\n",
      "POB\n",
      "FFM\n",
      "FIC\n",
      "FAQ\n",
      "FLG\n",
      "FNT\n",
      "FLO\n",
      "FNL\n",
      "WRI\n",
      "FOD\n",
      "FQW\n",
      "FHU\n",
      "TBN\n",
      "RSW\n",
      "FPR\n",
      "FSI\n",
      "FSM\n",
      "FWA\n",
      "FWH\n",
      "FYU\n",
      "FKL\n",
      "VZE\n",
      "FAT\n",
      "FRD\n",
      "FBS\n",
      "FNR\n",
      "GNV\n",
      "GVL\n",
      "GBH\n",
      "GAL\n",
      "GUP\n",
      "GAM\n",
      "GEK\n",
      "GCK\n",
      "GYY\n",
      "GCC\n",
      "AQY\n",
      "GGW\n",
      "AZ3\n",
      "GDV\n",
      "AK6\n",
      "FVW\n",
      "GLV\n",
      "GNU\n",
      "GYR\n",
      "FVX\n",
      "JGC\n",
      "GCN\n",
      "AZ1\n",
      "GFK\n",
      "GRI\n",
      "GJT\n",
      "GRR\n",
      "GPZ\n",
      "VWZ\n",
      "GMT\n",
      "XWA\n",
      "KGX\n",
      "GBD\n",
      "GTF\n",
      "GRB\n",
      "GSO\n",
      "GLH\n",
      "PGV\n",
      "GVT\n",
      "GSP\n",
      "UAM\n",
      "GUM\n",
      "GUF\n",
      "GPT\n",
      "GKN\n",
      "GUC\n",
      "GST\n",
      "HGR\n",
      "HNS\n",
      "A03\n",
      "HNM\n",
      "CMX\n",
      "VWD\n",
      "ZXJ\n",
      "HRL\n",
      "MDT\n",
      "HRO\n",
      "BDL\n",
      "PIB\n",
      "HVR\n",
      "HWI\n",
      "HHR\n",
      "HDN\n",
      "HYS\n",
      "HKB\n",
      "HLN\n",
      "T2X\n",
      "HES\n",
      "HIB\n",
      "HKY\n",
      "HIO\n",
      "ITO\n",
      "HHH\n",
      "HBH\n",
      "HOB\n",
      "HGZ\n",
      "HOL\n",
      "HYL\n",
      "HCR\n",
      "HOM\n",
      "HST\n",
      "VWX\n",
      "HNL\n",
      "MKK\n",
      "HNH\n",
      "HPB\n",
      "HOP\n",
      "HOT\n",
      "DWH\n",
      "EFD\n",
      "HOU\n",
      "HUS\n",
      "HSV\n",
      "HON\n",
      "HSL\n",
      "HUT\n",
      "HYA\n",
      "HYG\n",
      "WHD\n",
      "ICY\n",
      "IDA\n",
      "IGG\n",
      "ILI\n",
      "ZXF\n",
      "IND\n",
      "MQJ\n",
      "INL\n",
      "A57\n",
      "IYK\n",
      "IMT\n",
      "IWD\n",
      "ISP\n",
      "SAW\n",
      "ITH\n",
      "KIB\n",
      "A59\n",
      "A26\n",
      "MKL\n",
      "JAC\n",
      "JAN\n",
      "NZC\n",
      "JAX\n",
      "NIP\n",
      "OAJ\n",
      "JMS\n",
      "JHW\n",
      "VZM\n",
      "JON\n",
      "JST\n",
      "JBR\n",
      "JLN\n",
      "JNU\n",
      "OGG\n",
      "KAE\n",
      "A37\n",
      "A35\n",
      "KKK\n",
      "AZO\n",
      "LUP\n",
      "FCA\n",
      "KLG\n",
      "KAL\n",
      "MUE\n",
      "KNB\n",
      "MKC\n",
      "MCI\n",
      "JHM\n",
      "JRF\n",
      "KKL\n",
      "A65\n",
      "KYK\n",
      "KXA\n",
      "KUK\n",
      "VZR\n",
      "VZY\n",
      "FQD\n",
      "VIK\n",
      "MVM\n",
      "EAR\n",
      "EEN\n",
      "ENA\n",
      "KEH\n",
      "KTN\n",
      "WFB\n",
      "DQU\n",
      "EYW\n",
      "NQX\n",
      "QQB\n",
      "IAN\n",
      "GRK\n",
      "ILE\n",
      "A29\n",
      "KVC\n",
      "AKN\n",
      "IGM\n",
      "ISO\n",
      "KPN\n",
      "IRK\n",
      "KKB\n",
      "KVL\n",
      "KZH\n",
      "06A\n",
      "LMT\n",
      "KLW\n",
      "SZL\n",
      "TYS\n",
      "OBU\n",
      "A43\n",
      "ADQ\n",
      "KDK\n",
      "A41\n",
      "KNK\n",
      "KGK\n",
      "KOA\n",
      "KKH\n",
      "KOT\n",
      "OTZ\n",
      "KKA\n",
      "KYU\n",
      "LKK\n",
      "UUK\n",
      "KWT\n",
      "KWK\n",
      "LSE\n",
      "LAF\n",
      "LFT\n",
      "LCH\n",
      "XXW\n",
      "HII\n",
      "LMA\n",
      "TVL\n",
      "LNY\n",
      "ZXK\n",
      "WJF\n",
      "LNS\n",
      "LAN\n",
      "LAR\n",
      "LRD\n",
      "KLN\n",
      "HSH\n",
      "LSV\n",
      "VGT\n",
      "LBE\n",
      "LZU\n",
      "LAW\n",
      "ALZ\n",
      "LEB\n",
      "VA4\n",
      "KLL\n",
      "LWB\n",
      "LWS\n",
      "LEW\n",
      "LWT\n",
      "LEX\n",
      "LBL\n",
      "LIH\n",
      "UXA\n",
      "LVD\n",
      "LNK\n",
      "LIT\n",
      "05A\n",
      "LGU\n",
      "LNI\n",
      "LGB\n",
      "LIJ\n",
      "GGG\n",
      "LPS\n",
      "LPR\n",
      "LAM\n",
      "SDF\n",
      "LBB\n",
      "LYH\n",
      "MCN\n",
      "MSN\n",
      "A75\n",
      "MMH\n",
      "MNZ\n",
      "MHT\n",
      "MHK\n",
      "MBL\n",
      "MLY\n",
      "KMO\n",
      "MZJ\n",
      "MTH\n",
      "MYH\n",
      "MWA\n",
      "MQT\n",
      "MLL\n",
      "MVY\n",
      "MCW\n",
      "MSS\n",
      "MYK\n",
      "MAZ\n",
      "MYL\n",
      "MXY\n",
      "MCK\n",
      "OQA\n",
      "MCG\n",
      "MCL\n",
      "MFR\n",
      "MDR\n",
      "MYU\n",
      "MLB\n",
      "OQL\n",
      "MEM\n",
      "XXX\n",
      "MCE\n",
      "MEI\n",
      "OQM\n",
      "MFH\n",
      "MTM\n",
      "WMK\n",
      "MPB\n",
      "6B0\n",
      "MDO\n",
      "MAF\n",
      "MDY\n",
      "MLS\n",
      "NQA\n",
      "MKE\n",
      "MHM\n",
      "MWL\n",
      "STP\n",
      "MIB\n",
      "MOT\n",
      "MNT\n",
      "MFE\n",
      "MSO\n",
      "CNY\n",
      "BFM\n",
      "MOB\n",
      "MOD\n",
      "VZG\n",
      "MLI\n",
      "MLU\n",
      "MRY\n",
      "MGM\n",
      "MTJ\n",
      "UXR\n",
      "MGW\n",
      "MMU\n",
      "MVL\n",
      "KMY\n",
      "MWH\n",
      "MOS\n",
      "CWA\n",
      "MUO\n",
      "NUQ\n",
      "MOU\n",
      "A13\n",
      "MSL\n",
      "VZC\n",
      "MKG\n",
      "MYR\n",
      "NNK\n",
      "WQR\n",
      "AA2\n",
      "ACK\n",
      "KEB\n",
      "APC\n",
      "WNA\n",
      "KPM\n",
      "PKA\n",
      "APF\n",
      "BNA\n",
      "NKI\n",
      "NLG\n",
      "ENN\n",
      "EWB\n",
      "EWN\n",
      "HVN\n",
      "ARA\n",
      "GON\n",
      "NEW\n",
      "MSY\n",
      "DQN\n",
      "KNW\n",
      "JRB\n",
      "TSS\n",
      "NYC\n",
      "SWF\n",
      "LFI\n",
      "PHF\n",
      "ONP\n",
      "WWT\n",
      "EWK\n",
      "IAG\n",
      "NME\n",
      "NIB\n",
      "IKO\n",
      "NIN\n",
      "RQI\n",
      "WTK\n",
      "OME\n",
      "NNL\n",
      "ORV\n",
      "OFK\n",
      "ORF\n",
      "NGU\n",
      "OTH\n",
      "LBF\n",
      "MA5\n",
      "OHC\n",
      "ORT\n",
      "NUI\n",
      "NUL\n",
      "NUP\n",
      "ZNC\n",
      "ODW\n",
      "OAK\n",
      "OCF\n",
      "OFU\n",
      "HIF\n",
      "OGD\n",
      "OGS\n",
      "OKC\n",
      "OJC\n",
      "JCI\n",
      "OLH\n",
      "KOY\n",
      "XWS\n",
      "OLV\n",
      "OLM\n",
      "OMA\n",
      "ONN\n",
      "ONT\n",
      "OPH\n",
      "ORL\n",
      "OSH\n",
      "KOZ\n",
      "OWB\n",
      "UOX\n",
      "OXR\n",
      "PBK\n",
      "PAH\n",
      "PGA\n",
      "PPG\n",
      "PCE\n",
      "PSP\n",
      "PMD\n",
      "PAQ\n",
      "PFN\n",
      "ECP\n",
      "PAM\n",
      "PKD\n",
      "PKB\n",
      "PSC\n",
      "PRB\n",
      "DQR\n",
      "1G4\n",
      "DQW\n",
      "WQJ\n",
      "PDB\n",
      "PEC\n",
      "PLN\n",
      "PDT\n",
      "PNS\n",
      "NPA\n",
      "PIA\n",
      "KPV\n",
      "VYS\n",
      "GUS\n",
      "PSG\n",
      "PNF\n",
      "PNE\n",
      "LUF\n",
      "DVT\n",
      "AZA\n",
      "SCF\n",
      "PIR\n",
      "PIP\n",
      "UGB\n",
      "PQS\n",
      "SOP\n",
      "AGC\n",
      "PIT\n",
      "PSF\n",
      "PTU\n",
      "PLB\n",
      "PBG\n",
      "PTR\n",
      "PIH\n",
      "A27\n",
      "KPB\n",
      "PHO\n",
      "PIZ\n",
      "POQ\n",
      "PNC\n",
      "PSE\n",
      "PTK\n",
      "PVY\n",
      "PTD\n",
      "PTC\n",
      "PTA\n",
      "CLM\n",
      "KPY\n",
      "KPC\n",
      "PGM\n",
      "PTH\n",
      "A48\n",
      "ORI\n",
      "PML\n",
      "PPV\n",
      "TWD\n",
      "A17\n",
      "KPR\n",
      "PCA\n",
      "WQU\n",
      "PTV\n",
      "PWM\n",
      "PSM\n",
      "PRC\n",
      "PQI\n",
      "PUC\n",
      "BLF\n",
      "PPC\n",
      "PVD\n",
      "PVC\n",
      "PVU\n",
      "PUO\n",
      "A39\n",
      "PUB\n",
      "PUW\n",
      "OQP\n",
      "PGD\n",
      "AK5\n",
      "UIN\n",
      "KWN\n",
      "RDU\n",
      "RMP\n",
      "RCA\n",
      "RAP\n",
      "RDG\n",
      "RDV\n",
      "RDB\n",
      "A76\n",
      "A04\n",
      "RDR\n",
      "RDD\n",
      "RNO\n",
      "RNT\n",
      "RHI\n",
      "RIC\n",
      "RIL\n",
      "RIV\n",
      "RIW\n",
      "ROA\n",
      "RCE\n",
      "RST\n",
      "ROC\n",
      "RKS\n",
      "RFD\n",
      "RKD\n",
      "RWI\n",
      "ROG\n",
      "FAL\n",
      "RME\n",
      "RSJ\n",
      "ROW\n",
      "ROP\n",
      "RBY\n",
      "RUI\n",
      "RSH\n",
      "RSN\n",
      "RUT\n",
      "SAC\n",
      "SMF\n",
      "SAD\n",
      "MBS\n",
      "SPN\n",
      "SLE\n",
      "SLT\n",
      "SLN\n",
      "SNS\n",
      "SBY\n",
      "SMN\n",
      "ZXM\n",
      "SJT\n",
      "SKF\n",
      "SAT\n",
      "NKX\n",
      "MYF\n",
      "NZY\n",
      "SJC\n",
      "WSJ\n",
      "SIG\n",
      "SJU\n",
      "SBP\n",
      "SDP\n",
      "KSR\n",
      "OQS\n",
      "SFB\n",
      "SNA\n",
      "SBA\n",
      "SAF\n",
      "SMX\n",
      "STS\n",
      "SLK\n",
      "SRQ\n",
      "CIU\n",
      "SVN\n",
      "SAV\n",
      "SVA\n",
      "SCM\n",
      "BFF\n",
      "AVP\n",
      "SYB\n",
      "BFI\n",
      "LKE\n",
      "SDX\n",
      "A07\n",
      "WLK\n",
      "SOV\n",
      "A31\n",
      "SQV\n",
      "SWD\n",
      "SHX\n",
      "SKK\n",
      "A90\n",
      "A77\n",
      "SXP\n",
      "SYA\n",
      "SHR\n",
      "OQV\n",
      "SHH\n",
      "SOW\n",
      "BAD\n",
      "SHV\n",
      "SHG\n",
      "SDY\n",
      "SVC\n",
      "SUX\n",
      "FSD\n",
      "NKT\n",
      "SIT\n",
      "SKJ\n",
      "SGY\n",
      "SKW\n",
      "SLQ\n",
      "SCJ\n",
      "MQY\n",
      "SXQ\n",
      "SBN\n",
      "WSN\n",
      "SVW\n",
      "GEG\n",
      "SPI\n",
      "SGF\n",
      "UST\n",
      "STC\n",
      "STG\n",
      "SGU\n",
      "STJ\n",
      "CPS\n",
      "STL\n",
      "SUS\n",
      "KSM\n",
      "SMK\n",
      "SNP\n",
      "PIE\n",
      "RMN\n",
      "STF\n",
      "SCE\n",
      "SHD\n",
      "WSB\n",
      "WBB\n",
      "WA6\n",
      "VZO\n",
      "SVS\n",
      "SWO\n",
      "SCK\n",
      "SRV\n",
      "SSC\n",
      "SUN\n",
      "SYR\n",
      "TCM\n",
      "TIW\n",
      "TCT\n",
      "TKA\n",
      "TLH\n",
      "MCF\n",
      "TAL\n",
      "TSM\n",
      "TLJ\n",
      "TEK\n",
      "TAV\n",
      "TWE\n",
      "TLF\n",
      "TLA\n",
      "TEX\n",
      "TKE\n",
      "HUF\n",
      "A30\n",
      "TEB\n",
      "TEH\n",
      "TXK\n",
      "DLS\n",
      "TVF\n",
      "KTB\n",
      "TNC\n",
      "TIQ\n",
      "TOG\n",
      "TKJ\n",
      "TKI\n",
      "OOK\n",
      "TOL\n",
      "TPH\n",
      "FOE\n",
      "JZE\n",
      "TVC\n",
      "TTN\n",
      "TTD\n",
      "TUS\n",
      "TUL\n",
      "TLT\n",
      "UTM\n",
      "WTL\n",
      "TNK\n",
      "TUP\n",
      "TCL\n",
      "TWF\n",
      "TWA\n",
      "TYR\n",
      "TYE\n",
      "UGI\n",
      "UGS\n",
      "UMT\n",
      "UMB\n",
      "UNK\n",
      "DUT\n",
      "UTO\n",
      "VDZ\n",
      "VLD\n",
      "VPS\n",
      "VPZ\n",
      "VNY\n",
      "VUO\n",
      "VEE\n",
      "VEL\n",
      "VRB\n",
      "VCT\n",
      "VCV\n",
      "A67\n",
      "VQS\n",
      "VCB\n",
      "A70\n",
      "VIS\n",
      "OQI\n",
      "CNW\n",
      "ACT\n",
      "AIN\n",
      "AWK\n",
      "WAA\n",
      "ALW\n",
      "WWA\n",
      "OXC\n",
      "KWF\n",
      "ALO\n",
      "ART\n",
      "ATY\n",
      "EAT\n",
      "ENV\n",
      "VT1\n",
      "AWM\n",
      "PBI\n",
      "KWP\n",
      "WYS\n",
      "WST\n",
      "BAF\n",
      "FOK\n",
      "WSX\n",
      "WWP\n",
      "WMO\n",
      "HPN\n",
      "DQS\n",
      "SPS\n",
      "ICT\n",
      "WDB\n",
      "VZN\n",
      "IPT\n",
      "ISN\n",
      "WOW\n",
      "ILG\n",
      "ILM\n",
      "ILN\n",
      "WGO\n",
      "INW\n",
      "INT\n",
      "WA5\n",
      "WSM\n",
      "OLF\n",
      "ORH\n",
      "WRL\n",
      "WRG\n",
      "YKM\n",
      "YAK\n",
      "XWC\n",
      "WYB\n",
      "YNG\n",
      "A63\n",
      "NYL\n",
      "YUM\n",
      "KZB\n",
      "AK8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 162 of the file /usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# Data Wrangling Procedure \n",
    "# (For this example)\n",
    "# - Build a list of carrier values\n",
    "# - Build a list of airport values \n",
    "# - Make http request to download all data \n",
    "# - Then parse the data files \n",
    "\n",
    "# Best Practices for web scraping: \n",
    "# 1. Look at how a browser makes request\n",
    "# 2. emulate in code \n",
    "# 3. if stuff blows up, look at your Http traffic \n",
    "# 4. Return to 1 until it works. \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from lxml import etree\n",
    "\n",
    "def options(soup, id): \n",
    "    option_values = []\n",
    "    carrier_list = soup.find(id=id)\n",
    "    for option in carrier_list.find_all('option'): \n",
    "        option_values.append(option['value'])\n",
    "    return option_values \n",
    "\n",
    "def print_list(label,codes): \n",
    "    print \"\\n%s:\" % label\n",
    "    for c in codes: \n",
    "        print c \n",
    "\n",
    "def main(): \n",
    "    soup = BeautifulSoup(open(\"virgin_and_logan_airport.html\")) #Sacrament SouthWest Flights\n",
    "    codes = options(soup,'CarrierList')\n",
    "    print_list('Carriers',codes)\n",
    "    \n",
    "    codes = options(soup,'AirportList')\n",
    "    print_list('Airports', codes)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scraping a solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "html_page = \"virgin_and_logan_airport.html\"\n",
    "\n",
    "def extract_data(page): \n",
    "    data = {'eventvalidation': \"\", \n",
    "            \"viewstate\": \"\"}\n",
    "    \n",
    "    with open(page,'r') as html: \n",
    "        soup = BeautifulSoup(html,'lxml')\n",
    "        ev = soup.find(id='__EVENTVALIDATION')\n",
    "        data['eventvalidation'] = ev['value']\n",
    "        \n",
    "        vs = soup.find(id='__VIEWSTATE')\n",
    "        data['viewstate'] = vs['value']\n",
    "        \n",
    "    return data\n",
    "\n",
    "def make_request(data): \n",
    "    eventvalidation = data['eventvalidation']\n",
    "    viewstate = data['viewstate']\n",
    "    r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                    data={'AirportList': \"BOS\",\n",
    "                          'CarrierList': \"VX\",\n",
    "                          'Submit': 'Submit',\n",
    "                          \"__EVENTTARGET\": \"\",\n",
    "                          \"__EVENTARGUMENT\": \"\",\n",
    "                          \"__EVENTVALIDATION\": eventvalidation,\n",
    "                          \"__VIEWSTATE\": viewstate\n",
    "                    })\n",
    "\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    data = extract_data(html_page)\n",
    "    assert data[\"eventvalidation\"] != \"\"\n",
    "    assert data[\"eventvalidation\"].startswith(\"/wEWjAkCoIj1ng0\")\n",
    "    assert data[\"viewstate\"].startswith(\"/wEPDwUKLTI\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = requests.get('http://www.transtats.bts.gov/Data_Elements.aspx?Data=2')\n",
    "soup= BeautifulSoup(r.text)\n",
    "viewstate_element = soup.find(id=\"__VIEWSTATE\")\n",
    "viewstate = viewstate_element['value']\n",
    "eventvalidation_element = soup.find(id='__EVENTVALIDATION')\n",
    "eventvalidation = eventvalidation_element['value']\n",
    "r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                    data={'AirportList': \"BOS\",\n",
    "                          'CarrierList': \"VX\",\n",
    "                          'Submit': 'Submit',\n",
    "                          \"__EVENTTARGET\": \"\",\n",
    "                          \"__EVENTARGUMENT\": \"\",\n",
    "                          \"__EVENTVALIDATION\": eventvalidation,\n",
    "                          \"__VIEWSTATE\": viewstate\n",
    "                    })\n",
    "\n",
    "f = open('virgin_and_logan_airport.html','w')\n",
    "f.write(r.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# s = requests.Session() \n",
    "# r = s.post(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "#            data = (\n",
    "#                    (\"__EVENTTARGET\", \"\"),\n",
    "#                    (\"__EVENTARGUMENT\", \"\"),\n",
    "#                    (\"__VIEWSTATE\", viewstate),\n",
    "#                    (\"__VIEWSTATEGENERATOR\",viewstategenerator),\n",
    "#                    (\"__EVENTVALIDATION\", eventvalidation),\n",
    "#                    (\"CarrierList\", \"VX\"),\n",
    "#                    (\"AirportList\", \"BOS\"),\n",
    "#                    (\"Submit\", \"Submit\")\n",
    "#                   ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Quiz Questions: \n",
    "\n",
    "### Question 1: \n",
    "\"\"\"\n",
    "Your task in this exercise is to modify 'extract_carrier()` to get a list of\n",
    "all airlines. Exclude all of the combination values like \"All U.S. Carriers\"\n",
    "from the data that you return. You should return a list of codes for the\n",
    "carriers.\n",
    "\n",
    "All your changes should be in the 'extract_carrier()' function. The\n",
    "'options.html' file in the tab above is a stripped down version of what is\n",
    "actually on the website, but should provide an example of what you should get\n",
    "from the full file.\n",
    "\n",
    "Please note that the function 'make_request()' is provided for your reference\n",
    "only. You will not be able to to actually use it from within the Udacity web UI.\n",
    "\"\"\"\n",
    "\n",
    "### Question 2\n",
    "\"\"\"\n",
    "Complete the 'extract_airports()' function so that it returns a list of airport\n",
    "codes, excluding any combinations like \"All\".\n",
    "\n",
    "Refer to the 'options.html' file in the tab above for a stripped down version\n",
    "of what is actually on the website. The test() assertions are based on the\n",
    "given file.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "### Question 3 \n",
    "\"\"\"\n",
    "Let's assume that you combined the code from the previous 2 exercises with code\n",
    "from the lesson on how to build requests, and downloaded all the data locally.\n",
    "The files are in a directory \"data\", named after the carrier and airport:\n",
    "\"{}-{}.html\".format(carrier, airport), for example \"FL-ATL.html\".\n",
    "\n",
    "The table with flight info has a table class=\"dataTDRight\". Your task is to\n",
    "use 'process_file()' to extract the flight data from that table as a list of\n",
    "dictionaries, each dictionary containing relevant data from the file and table\n",
    "row. This is an example of the data structure you should return:\n",
    "\n",
    "data = [{\"courier\": \"FL\",\n",
    "         \"airport\": \"ATL\",\n",
    "         \"year\": 2012,\n",
    "         \"month\": 12,\n",
    "         \"flights\": {\"domestic\": 100,\n",
    "                     \"international\": 100}\n",
    "        },\n",
    "         {\"courier\": \"...\"}\n",
    "]\n",
    "\n",
    "Note - year, month, and the flight data should be integers.\n",
    "You should skip the rows that contain the TOTAL data for a year.\n",
    "\n",
    "There are couple of helper functions to deal with the data files.\n",
    "Please do not change them for grading purposes.\n",
    "All your changes should be in the 'process_file()' function.\n",
    "\n",
    "The 'data/FL-ATL.html' file in the tab above is only a part of the full data,\n",
    "covering data through 2003. The test() code will be run on the full table, but\n",
    "the given file should provide an example of what you will get.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "#Question 1\n",
    "html_page = 'Data_Elements.aspx.html'\n",
    "def extract_carriers(page):\n",
    "    data = []\n",
    "\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        carrier_list = soup.find(id ='CarrierList')\n",
    "        for option in carrier_list.find_all('option'): \n",
    "            data.append(option['value'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Question 2 \n",
    "\n",
    "def extract_airports(page): \n",
    "    data = []\n",
    "    with open(page,'r') as html: \n",
    "        soup = BeautifulSoup(html,'lxml')\n",
    "        airport_list = soup.find(id = \"AirportList\")\n",
    "        for option in airport_list.find_all('option'): \n",
    "            data.append(option['value'])\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "datadir = \"data\"\n",
    "\n",
    "\n",
    "def open_zip(datadir):\n",
    "    with ZipFile('{0}.zip'.format(datadir), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def process_all(datadir):\n",
    "    files = os.listdir(datadir)\n",
    "    return files\n",
    "\n",
    "\n",
    "def process_file(f):\n",
    "    \"\"\"\n",
    "    This function extracts data from the file given as the function argument in\n",
    "    a list of dictionaries. This is example of the data structure you should\n",
    "    return:\n",
    "\n",
    "    data = [{\"courier\": \"FL\",\n",
    "             \"airport\": \"ATL\",\n",
    "             \"year\": 2012,\n",
    "             \"month\": 12,\n",
    "             \"flights\": {\"domestic\": 100,\n",
    "                         \"international\": 100}\n",
    "            },\n",
    "            {\"courier\": \"...\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "    Note - year, month, and the flight data should be integers.\n",
    "    You should skip the rows that contain the TOTAL data for a year.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    info = {}\n",
    "    info[\"courier\"], info[\"airport\"] = f[:6].split(\"-\")\n",
    "    # Note: create a new dictionary for each entry in the output data list.\n",
    "    # If you use the info dictionary defined here each element in the list \n",
    "    # will be a reference to the same info dictionary.\n",
    "    with open(\"{}/{}\".format(datadir, f), \"r\") as html:\n",
    "\n",
    "        soup = BeautifulSoup(html)\n",
    "        data_table = soup.find('table',class_ = 'dataTDRight')\n",
    "        \n",
    "        for row in data_table.find_all('tr'): \n",
    "            cells = row.find_all('td')\n",
    "            if cells[1].text == 'Total': \n",
    "                continue \n",
    "            else: \n",
    "                info['courier'] = info['courier']\n",
    "                info['airport'] = info['airport']\n",
    "                info['year'] = cells[0].text\n",
    "                info['month'] = cells[1].text\n",
    "                info[\"flights\"] = {'domestic': str(cells[2].text.replace(',', '')),\n",
    "                                    'international': str(cells[3].text.replace(',', ''))}\n",
    "    print info\n",
    "    return data.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}]\n"
     ]
    }
   ],
   "source": [
    "import pprint \n",
    "data = []\n",
    "info = {}\n",
    "with open(html_page, \"r\") as html: \n",
    "    soup = BeautifulSoup(html)\n",
    "    airport_carrier = soup.find('table', class_='dataTDRight')\n",
    "    for row in airport_carrier.find_all('tr')[1:]: #skip the first header row\n",
    "        cells = row.find_all('td')\n",
    "        if cells[1].text == 'TOTAL': \n",
    "            continue\n",
    "        else: \n",
    "            info['courier'] = 0\n",
    "            info['airport'] = 0 \n",
    "            info['year'] = int(cells[0].text)\n",
    "            info['month'] = int(cells[1].text)\n",
    "            info['flights'] = {'domestic': int(str(cells[2].text.replace(',',''))), \n",
    "                                \"international\": int(str(cells[3].text.replace(',','')))}\n",
    "            data.append(info)\n",
    "print data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#regular expression module \n",
    "#xml etree module \n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_file(input_file, output_good, output_bad):\n",
    "    # store data into lists for output\n",
    "    data_good = []\n",
    "    data_bad = []\n",
    "    with open(input_file, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        header = reader.fieldnames\n",
    "        for row in reader:\n",
    "            # validate URI value\n",
    "            if row['URI'].find(\"dbpedia.org\") < 0:\n",
    "                continue\n",
    "\n",
    "            ps_year = row['productionStartYear'][:4]\n",
    "            try: # use try/except to filter valid items\n",
    "                ps_year = int(ps_year)\n",
    "                row['productionStartYear'] = ps_year\n",
    "                if (ps_year >= 1886) and (ps_year <= 2014):\n",
    "                    data_good.append(row)\n",
    "                else:\n",
    "                    data_bad.append(row)\n",
    "            except ValueError: # non-numeric strings caught by exception\n",
    "                if ps_year == 'NULL':\n",
    "                    data_bad.append(row)\n",
    "\n",
    "    # Write processed data to output files\n",
    "    with open(output_good, \"w\") as good:\n",
    "        writer = csv.DictWriter(good, delimiter=\",\", fieldnames= header)\n",
    "        writer.writeheader()\n",
    "        for row in data_good:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    with open(output_bad, \"w\") as bad:\n",
    "        writer = csv.DictWriter(bad, delimiter=\",\", fieldnames= header)\n",
    "        writer.writeheader()\n",
    "        for row in data_bad:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Audit.py \n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "In this problem set you work with cities infobox data, audit it, come up with a\n",
    "cleaning idea and then clean it up. In the first exercise we want you to audit\n",
    "the datatypes that can be found in some particular fields in the dataset.\n",
    "The possible types of values can be:\n",
    "- NoneType if the value is a string \"NULL\" or an empty string \"\"\n",
    "- list, if the value starts with \"{\"\n",
    "- int, if the value can be cast to int\n",
    "- float, if the value can be cast to float, but CANNOT be cast to int.\n",
    "   For example, '3.23e+07' should be considered a float because it can be cast\n",
    "   as float but int('3.23e+07') will throw a ValueError\n",
    "- 'str', for all other values\n",
    "\n",
    "The audit_file function should return a dictionary containing fieldnames and a \n",
    "SET of the types that can be found in the field. e.g.\n",
    "{\"field1\": set([type(float()), type(int()), type(str())]),\n",
    " \"field2\": set([type(str())]),\n",
    "  ....\n",
    "}\n",
    "The type() function returns a type object describing the argument given to the \n",
    "function. You can also use examples of objects to create type objects, e.g.\n",
    "type(1.1) for a float: see the test function below for examples.\n",
    "\n",
    "Note that the first three rows (after the header row) in the cities.csv file\n",
    "are not actual data points. The contents of these rows should note be included\n",
    "when processing data types. Be sure to include functionality in your code to\n",
    "skip over or detect these rows.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "import pprint\n",
    "FIELDS = [\"name\", \"timeZone_label\", \"utcOffset\", \"homepage\", \"governmentType_label\",\n",
    "          \"isPartOf_label\", \"areaCode\", \"populationTotal\", \"elevation\",\n",
    "          \"maximumElevation\", \"minimumElevation\", \"populationDensity\",\n",
    "          \"wgs84_pos#lat\", \"wgs84_pos#long\", \"areaLand\", \"areaMetro\", \"areaUrban\"]\n",
    "fieldtypes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter an integer: 3.2\n",
      "No valid integer! Please try again ...\n",
      "Please enter an integer: 3\n",
      "Great, you successfully entered an integer!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        n = raw_input(\"Please enter an integer: \")\n",
    "        n = int(n)\n",
    "        break\n",
    "    except ValueError:\n",
    "        print(\"No valid integer! Please try again ...\")\n",
    "print \"Great, you successfully entered an integer!\"\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fieldtypes[\"areaLand\"] = set([type(1.1),type([]), type(None)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{float, list, NoneType}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fieldtypes['areaLand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_int(float(\"3.23e+07\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32300000.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float('3.23e+07')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_number(s): \n",
    "    try: \n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError: \n",
    "        return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'areaCode': set([]),\n",
      " 'areaLand': set([]),\n",
      " 'areaMetro': set([]),\n",
      " 'areaUrban': set([<type 'float'>, <type 'NoneType'>, <type 'str'>]),\n",
      " 'elevation': set([]),\n",
      " 'governmentType_label': set([]),\n",
      " 'homepage': set([]),\n",
      " 'isPartOf_label': set([]),\n",
      " 'maximumElevation': set([]),\n",
      " 'minimumElevation': set([]),\n",
      " 'name': set([]),\n",
      " 'populationDensity': set([]),\n",
      " 'populationTotal': set([]),\n",
      " 'timeZone_label': set([]),\n",
      " 'utcOffset': set([]),\n",
      " 'wgs84_pos#lat': set([]),\n",
      " 'wgs84_pos#long': set([])}\n"
     ]
    }
   ],
   "source": [
    "fieldtypes = {}\n",
    "for field in FIELDS: \n",
    "    fieldtypes[field] = set() \n",
    "with open('cities.csv','r') as f: \n",
    "    reader = csv.DictReader(f)\n",
    "    for i in range(3): \n",
    "        reader.next()\n",
    "    for row in reader: \n",
    "        for fields in FIELDS: \n",
    "            if row[fields] == 'NULL' or row[fields] == \"\" or row[fields] == None: \n",
    "                fieldtypes[field].add(type(None))\n",
    "            if row[fields] == \"{\": \n",
    "                fieldtypes[field].add(type([]))\n",
    "            else: \n",
    "                try: \n",
    "                    int(row[field])\n",
    "                    fieldtypes.add(type(1))\n",
    "                except ValueError: \n",
    "                    try: \n",
    "                        float(row[field])\n",
    "                        fieldtypes[field].add(type(1.1))\n",
    "                    except ValueError:\n",
    "                        fieldtypes[field].add(type('string'))\n",
    "pprint.pprint(fieldtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row[fields].find('}') # zero if found -1 otherwise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audit_file(filename, fields):\n",
    "    fieldtypes = {}\n",
    "    for fields in FIELDS: \n",
    "        fieldtypes[fields] = set() # make a set for the fields\n",
    "    with open(filename,'r') as f: \n",
    "        reader = csv.DictReader(f) # create a dictionary reader\n",
    "        for i in range(3): \n",
    "            reader.next() # skip the first three files \n",
    "        for row in reader: \n",
    "            for fields in FIELDS: \n",
    "                if row[fields] == \"NULL\" or row[fields] == \"\": \n",
    "                    fieldtypes[fields].add(type(None)) \n",
    "                elif row[fields].find(\"{\") == 0: \n",
    "                    fieldtypes[fields].add(type([]))\n",
    "                else:\n",
    "                    try: \n",
    "                        int(row[fields]) \n",
    "                        fieldtypes[fields].add(type(1)) \n",
    "                    except ValueError: \n",
    "                        try: \n",
    "                            float(row[fields])\n",
    "                            fieldtypes[fields].add(type(1.1))\n",
    "                        except ValueError: \n",
    "                            fieldtypes[fields].add(type('string')) \n",
    "    return fieldtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'areaCode': {int, NoneType, str},\n",
       " 'areaLand': {float, list, NoneType},\n",
       " 'areaMetro': {float, NoneType},\n",
       " 'areaUrban': {float, NoneType},\n",
       " 'elevation': {float, list, NoneType},\n",
       " 'governmentType_label': {NoneType, str},\n",
       " 'homepage': {NoneType, str},\n",
       " 'isPartOf_label': {list, NoneType, str},\n",
       " 'maximumElevation': {NoneType},\n",
       " 'minimumElevation': {NoneType},\n",
       " 'name': {list, NoneType, str},\n",
       " 'populationDensity': {float, list, NoneType},\n",
       " 'populationTotal': {int, NoneType},\n",
       " 'timeZone_label': {NoneType, str},\n",
       " 'utcOffset': {int, list, NoneType, str},\n",
       " 'wgs84_pos#lat': {float},\n",
       " 'wgs84_pos#long': {float}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audit_file('cities.csv',FIELDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{1.20175e+07|1.202e+07}'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['areaLand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fix_area(area):\n",
    "    data_area = []\n",
    "    if area == 'NULL': \n",
    "        return None\n",
    "    else: \n",
    "        area = area.replace('{',\"\")\n",
    "        area = area.replace('}',\"\")\n",
    "        split_area = area.split(\"|\")\n",
    "        for num in split_area: \n",
    "            data_area.append(float(num))\n",
    "        return data_area\n",
    "\n",
    "def process_file(filename):\n",
    "    # CHANGES TO THIS FUNCTION WILL BE IGNORED WHEN YOU SUBMIT THE EXERCISE\n",
    "    data = []\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "\n",
    "        #skipping the extra metadata\n",
    "        for i in range(3):\n",
    "            l = reader.next()\n",
    "\n",
    "        # processing file\n",
    "        for line in reader:\n",
    "            # calling your function to fix the area value\n",
    "            if \"areaLand\" in line:\n",
    "                line[\"areaLand\"] = fix_area(line[\"areaLand\"])\n",
    "            data.append(line)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = process_file('cities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area = data[6]['areaLand'].replace(\"{\", \"\")\n",
    "area = area.replace(\"}\",\"\")\n",
    "split_area = area.split(\"|\")\n",
    "data_area = []\n",
    "for i in split_area: \n",
    "    data_area.append(float(i))\n",
    "len(data_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[55166700.0, 55300000.0]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[8]['areaLand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{1.20175e+07|1.202e+07}'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['areaLand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict \n",
    "import re \n",
    "import pprint\n",
    "\n",
    "#Passes test\n",
    "def count_tags(filename):\n",
    "    elemList = []\n",
    "    tree = ET.parse(filename) #Parse XML file\n",
    "    tags = {'bounds': 0,\n",
    "        \"member\": 0,\n",
    "        'nd': 0,\n",
    "        \"node\": 0, \n",
    "        \"osm\": 0,\n",
    "        \"relation\": 0,\n",
    "        \"tag\": 0,\n",
    "        \"way\": 0,\n",
    "       }\n",
    "    for elem in tree.iter():\n",
    "        elemList.append(elem.tag) #gives me all tags in XML file \n",
    "    \n",
    "    for key in elemList: \n",
    "        if key in tags: \n",
    "            tags[key] += 1 \n",
    "    return tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "#Passes Test \n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        k = element.attrib['k']\n",
    "        if re.search(lower,k) != None: \n",
    "            keys['lower'] += 1 \n",
    "        elif re.search(lower_colon,k) != None: \n",
    "            keys['lower_colon'] +=1 \n",
    "        elif re.search(problemchars,k) != None: \n",
    "            keys['problemchars'] +=1\n",
    "        else: \n",
    "            keys['other'] += 1\n",
    "    return keys\n",
    "\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#passes test \n",
    "def get_user(element):\n",
    "    if element.tag == 'node':\n",
    "        return element.attrib['uid']\n",
    "    if element.tag == 'way': \n",
    "        return element.attrib['uid']\n",
    "    if element.tag == 'relation': \n",
    "        return element.attrib['uid']\n",
    "    \n",
    "def process_map(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        users.add(get_user(element))\n",
    "        users.discard(None)\n",
    "    return users\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OSMFILE = 'example_2.osm'\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]\n",
    "\n",
    "# UPDATE THIS VARIABLE\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Ave\" : \"Avenue\",\n",
    "            \"Ave.\" : \"Avenue\",\n",
    "            \"Rd\" : \"Road\",\n",
    "            \"Rd.\" : \"Road\",\n",
    "            \"Blvd\" : \"Boulevard\",\n",
    "            \"Blvd.\" : \"Boulevard\",\n",
    "            \"Cir\" : \"Circle\",\n",
    "            \"Cir.\" : \"Circle\",\n",
    "            \"Ct\" : \"Court\",\n",
    "            \"Ct.\" : \"Court\",\n",
    "            \"Dr\" : \"Drive\",\n",
    "            \"Dr.\" : \"Drive\",\n",
    "            \"Pl\" : \"Place\",\n",
    "            \"Pl.\" : \"Place\",\n",
    "            }\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    \n",
    "        m = street_type_re.search(name)\n",
    "        if m not in expected:\n",
    "            name = re.sub(m.group(), mapping[m.group()], name)\n",
    "    \n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "After auditing is complete the next step is to prepare the data to be inserted into a SQL database.\n",
    "To do so you will parse the elements in the OSM XML file, transforming them from document format to\n",
    "tabular format, thus making it possible to write to .csv files.  These csv files can then easily be\n",
    "imported to a SQL database as tables.\n",
    "\n",
    "The process for this transformation is as follows:\n",
    "- Use iterparse to iteratively step through each top level element in the XML\n",
    "- Shape each element into several data structures using a custom function\n",
    "- Utilize a schema and validation library to ensure the transformed data is in the correct format\n",
    "- Write each data structure to the appropriate .csv files\n",
    "\n",
    "We've already provided the code needed to load the data, perform iterative parsing and write the\n",
    "output to csv files. Your task is to complete the shape_element function that will transform each\n",
    "element into the correct format. To make this process easier we've already defined a schema (see\n",
    "the schema.py file in the last code tab) for the .csv files and the eventual tables. Using the \n",
    "cerberus library we can validate the output against this schema to ensure it is correct.\n",
    "\n",
    "## Shape Element Function\n",
    "The function should take as input an iterparse Element object and return a dictionary.\n",
    "\n",
    "### If the element top level tag is \"node\":\n",
    "The dictionary returned should have the format {\"node\": .., \"node_tags\": ...}\n",
    "\n",
    "The \"node\" field should hold a dictionary of the following top level node attributes:\n",
    "- id\n",
    "- user\n",
    "- uid\n",
    "- version\n",
    "- lat\n",
    "- lon\n",
    "- timestamp\n",
    "- changeset\n",
    "All other attributes can be ignored\n",
    "\n",
    "The \"node_tags\" field should hold a list of dictionaries, one per secondary tag. Secondary tags are\n",
    "child tags of node which have the tag name/type: \"tag\". Each dictionary should have the following\n",
    "fields from the secondary tag attributes:\n",
    "- id: the top level node id attribute value\n",
    "- key: the full tag \"k\" attribute value if no colon is present or the characters after the colon if one is.\n",
    "- value: the tag \"v\" attribute value\n",
    "- type: either the characters before the colon in the tag \"k\" value or \"regular\" if a colon\n",
    "        is not present.\n",
    "\n",
    "Additionally,\n",
    "\n",
    "- if the tag \"k\" value contains problematic characters, the tag should be ignored\n",
    "- if the tag \"k\" value contains a \":\" the characters before the \":\" should be set as the tag type\n",
    "  and characters after the \":\" should be set as the tag key\n",
    "- if there are additional \":\" in the \"k\" value they and they should be ignored and kept as part of\n",
    "  the tag key. For example:\n",
    "\n",
    "  <tag k=\"addr:street:name\" v=\"Lincoln\"/>\n",
    "  should be turned into\n",
    "  {'id': 12345, 'key': 'street:name', 'value': 'Lincoln', 'type': 'addr'}\n",
    "\n",
    "- If a node has no secondary tags then the \"node_tags\" field should just contain an empty list.\n",
    "\n",
    "The final return value for a \"node\" element should look something like:\n",
    "\n",
    "{'node': {'id': 757860928,\n",
    "          'user': 'uboot',\n",
    "          'uid': 26299,\n",
    "       'version': '2',\n",
    "          'lat': 41.9747374,\n",
    "          'lon': -87.6920102,\n",
    "          'timestamp': '2010-07-22T16:16:51Z',\n",
    "      'changeset': 5288876},\n",
    " 'node_tags': [{'id': 757860928,\n",
    "                'key': 'amenity',\n",
    "                'value': 'fast_food',\n",
    "                'type': 'regular'},\n",
    "               {'id': 757860928,\n",
    "                'key': 'cuisine',\n",
    "                'value': 'sausage',\n",
    "                'type': 'regular'},\n",
    "               {'id': 757860928,\n",
    "                'key': 'name',\n",
    "                'value': \"Shelly's Tasty Freeze\",\n",
    "                'type': 'regular'}]}\n",
    "\n",
    "### If the element top level tag is \"way\":\n",
    "The dictionary should have the format {\"way\": ..., \"way_tags\": ..., \"way_nodes\": ...}\n",
    "\n",
    "The \"way\" field should hold a dictionary of the following top level way attributes:\n",
    "- id\n",
    "-  user\n",
    "- uid\n",
    "- version\n",
    "- timestamp\n",
    "- changeset\n",
    "\n",
    "All other attributes can be ignored\n",
    "\n",
    "The \"way_tags\" field should again hold a list of dictionaries, following the exact same rules as\n",
    "for \"node_tags\".\n",
    "\n",
    "Additionally, the dictionary should have a field \"way_nodes\". \"way_nodes\" should hold a list of\n",
    "dictionaries, one for each nd child tag.  Each dictionary should have the fields:\n",
    "- id: the top level element (way) id\n",
    "- node_id: the ref attribute value of the nd tag\n",
    "- position: the index starting at 0 of the nd tag i.e. what order the nd tag appears within\n",
    "            the way element\n",
    "\n",
    "The final return value for a \"way\" element should look something like:\n",
    "\n",
    "{'way': {'id': 209809850,\n",
    "         'user': 'chicago-buildings',\n",
    "         'uid': 674454,\n",
    "         'version': '1',\n",
    "         'timestamp': '2013-03-13T15:58:04Z',\n",
    "         'changeset': 15353317},\n",
    " 'way_nodes': [{'id': 209809850, 'node_id': 2199822281, 'position': 0},\n",
    "               {'id': 209809850, 'node_id': 2199822390, 'position': 1},\n",
    "               {'id': 209809850, 'node_id': 2199822392, 'position': 2},\n",
    "               {'id': 209809850, 'node_id': 2199822369, 'position': 3},\n",
    "               {'id': 209809850, 'node_id': 2199822370, 'position': 4},\n",
    "               {'id': 209809850, 'node_id': 2199822284, 'position': 5},\n",
    "               {'id': 209809850, 'node_id': 2199822281, 'position': 6}],\n",
    " 'way_tags': [{'id': 209809850,\n",
    "               'key': 'housenumber',\n",
    "               'type': 'addr',\n",
    "               'value': '1412'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'street',\n",
    "               'type': 'addr',\n",
    "               'value': 'West Lexington St.'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'street:name',\n",
    "               'type': 'addr',\n",
    "               'value': 'Lexington'},\n",
    "              {'id': '209809850',\n",
    "               'key': 'street:prefix',\n",
    "               'type': 'addr',\n",
    "               'value': 'West'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'street:type',\n",
    "               'type': 'addr',\n",
    "               'value': 'Street'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'building',\n",
    "               'type': 'regular',\n",
    "               'value': 'yes'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'levels',\n",
    "               'type': 'building',\n",
    "               'value': '1'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'building_id',\n",
    "               'type': 'chicago',\n",
    "               'value': '366409'}]}\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "import cerberus\n",
    "\n",
    "import schema\n",
    "\n",
    "OSM_PATH = \"example_3.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    if element.tag == 'node':\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "# def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "#     \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "#     context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "#     _, root = next(context)\n",
    "#     for event, elem in context:\n",
    "#         if event == 'end' and elem.tag in tags:\n",
    "#             yield elem\n",
    "#             root.clear()\n",
    "\n",
    "\n",
    "# def validate_element(element, validator, schema=SCHEMA):\n",
    "#     \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "#     if validator.validate(element, schema) is not True:\n",
    "#         field, errors = next(validator.errors.iteritems())\n",
    "#         message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "#         error_string = pprint.pformat(errors)\n",
    "        \n",
    "#         raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "# class UnicodeDictWriter(csv.DictWriter, object):\n",
    "#     \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "#     def writerow(self, row):\n",
    "#         super(UnicodeDictWriter, self).writerow({\n",
    "#             k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "#         })\n",
    "\n",
    "#     def writerows(self, rows):\n",
    "#         for row in rows:\n",
    "#             self.writerow(row)\n",
    "\n",
    "\n",
    "# # ================================================== #\n",
    "# #               Main Function                        #\n",
    "# # ================================================== #\n",
    "# def process_map(file_in, validate):\n",
    "#     \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "#     with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "#          codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "#          codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "#          codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "#          codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "#         nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "#         node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "#         ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "#         way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "#         way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "#         nodes_writer.writeheader()\n",
    "#         node_tags_writer.writeheader()\n",
    "#         ways_writer.writeheader()\n",
    "#         way_nodes_writer.writeheader()\n",
    "#         way_tags_writer.writeheader()\n",
    "\n",
    "#         validator = cerberus.Validator()\n",
    "\n",
    "#         for element in get_element(file_in, tags=('node', 'way')):\n",
    "#             el = shape_element(element)\n",
    "#             if el:\n",
    "#                 if validate is True:\n",
    "#                     validate_element(el, validator)\n",
    "\n",
    "#                 if element.tag == 'node':\n",
    "#                     nodes_writer.writerow(el['node'])\n",
    "#                     node_tags_writer.writerows(el['node_tags'])\n",
    "#                 elif element.tag == 'way':\n",
    "#                     ways_writer.writerow(el['way'])\n",
    "#                     way_nodes_writer.writerows(el['way_nodes'])\n",
    "#                     way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "#     # sample of the map when validating.\n",
    "#     process_map(OSM_PATH, validate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '261221424',\n",
      "  'key': 'highway',\n",
      "  'type': None,\n",
      "  'value': 'traffic_signals'}]\n",
      "[{'id': '2406124091', 'key': 'addr:city', 'type': None, 'value': 'Chicago'}]\n",
      "[{'id': '2406124091',\n",
      "  'key': 'addr:housenumber',\n",
      "  'type': None,\n",
      "  'value': '5157'}]\n",
      "[{'id': '2406124091', 'key': 'addr:postcode', 'type': None, 'value': '60625'}]\n",
      "[{'id': '2406124091',\n",
      "  'key': 'addr:street',\n",
      "  'type': None,\n",
      "  'value': 'North Lincoln Ave'}]\n",
      "[{'id': '2406124091', 'key': 'amenity', 'type': None, 'value': 'restaurant'}]\n",
      "[{'id': '2406124091', 'key': 'cuisine', 'type': None, 'value': 'mexican'}]\n",
      "[{'id': '2406124091',\n",
      "  'key': 'name',\n",
      "  'type': None,\n",
      "  'value': 'La Cabana De Don Luis'}]\n",
      "[{'id': '2406124091', 'key': 'outdoor_seating', 'type': None, 'value': 'no'}]\n",
      "[{'id': '2406124091',\n",
      "  'key': 'phone',\n",
      "  'type': None,\n",
      "  'value': '1 (773)-271-5176'}]\n",
      "[{'id': '2406124091', 'key': 'smoking', 'type': None, 'value': 'no'}]\n",
      "[{'id': '2406124091', 'key': 'takeaway', 'type': None, 'value': 'yes'}]\n",
      "[{'id': '2636084635', 'key': 'addr:city', 'type': None, 'value': 'Chicago'}]\n",
      "[{'id': '2636084635', 'key': 'addr:country', 'type': None, 'value': 'US'}]\n",
      "[{'id': '2636084635',\n",
      "  'key': 'addr:housenumber',\n",
      "  'type': None,\n",
      "  'value': '4874'}]\n",
      "[{'id': '2636084635', 'key': 'addr:postcode', 'type': None, 'value': '60625'}]\n",
      "[{'id': '2636084635', 'key': 'addr:state', 'type': None, 'value': 'Illinois'}]\n",
      "[{'id': '2636084635',\n",
      "  'key': 'addr:street',\n",
      "  'type': None,\n",
      "  'value': 'N. Lincoln Ave'}]\n",
      "[{'id': '2636084635', 'key': 'name', 'type': None, 'value': 'Matty Ks'}]\n",
      "[{'id': '2636084635', 'key': 'phone', 'type': None, 'value': '(773)-654-1347'}]\n",
      "[{'id': '2636084635', 'key': 'shop', 'type': None, 'value': 'doityourself'}]\n",
      "[{'id': '2636084635', 'key': 'source', 'type': None, 'value': 'GPS'}]\n",
      "[{'id': '757860928', 'key': 'amenity', 'type': None, 'value': 'fast_food'}]\n",
      "[{'id': '757860928', 'key': 'cuisine', 'type': None, 'value': 'sausage'}]\n",
      "[{'id': '757860928',\n",
      "  'key': 'name',\n",
      "  'type': None,\n",
      "  'value': \"Shelly's Tasty Freeze\"}]\n",
      "[{'id': '1683602133',\n",
      "  'key': 'addr:housename',\n",
      "  'type': None,\n",
      "  'value': 'Village Hall'}]\n",
      "[{'id': '1683602133',\n",
      "  'key': 'addr:housenumber',\n",
      "  'type': None,\n",
      "  'value': '1400'}]\n",
      "[{'id': '1683602133', 'key': 'addr:postcode', 'type': None, 'value': '60067'}]\n",
      "[{'id': '1683602133',\n",
      "  'key': 'addr:street',\n",
      "  'type': None,\n",
      "  'value': 'Baldwin Rd.'}]\n",
      "[{'id': '1683602133', 'key': 'amenity', 'type': None, 'value': 'townhall'}]\n",
      "[{'id': '1683602133', 'key': 'name', 'type': None, 'value': 'Village Hall'}]\n"
     ]
    }
   ],
   "source": [
    "#let's work with the nodes first.\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "{'node': {'id': 757860928,\n",
    "          'user': 'uboot',\n",
    "          'uid': 26299,\n",
    "       'version': '2',\n",
    "          'lat': 41.9747374,\n",
    "          'lon': -87.6920102,\n",
    "          'timestamp': '2010-07-22T16:16:51Z',\n",
    "      'changeset': 5288876},\n",
    " 'node_tags': [{'id': 757860928,\n",
    "                'key': 'amenity',\n",
    "                'value': 'fast_food',\n",
    "                'type': 'regular'},\n",
    "               {'id': 757860928,\n",
    "                'key': 'cuisine',\n",
    "                'value': 'sausage',\n",
    "                'type': 'regular'},\n",
    "               {'id': 757860928,\n",
    "                'key': 'name',\n",
    "                'value': \"Shelly's Tasty Freeze\",\n",
    "                'type': 'regular'}]}\n",
    "\n",
    "\n",
    "tree = ET.parse(OSM_PATH)\n",
    "node_attributes = dict.fromkeys(NODE_FIELDS, None)\n",
    "tag = [dict.fromkeys(NODE_TAGS_FIELDS, None)]\n",
    "node_dic = {'node': None, \n",
    "             'node_tags': None}\n",
    "\n",
    "\n",
    "for elem in tree.iter(): \n",
    "    if elem.tag == 'node':\n",
    "        for attribute in NODE_FIELDS:\n",
    "            node_attributes[attribute] = elem.attrib[attribute]\n",
    "            node_dic['node'] = node_attributes\n",
    "        for tags in elem.iter('tag'):\n",
    "            tag[0]['id'] = elem.attrib['id']\n",
    "            tag[0]['key'] = tags.attrib['k']\n",
    "            tag[0]['value'] = tags.attrib['v']\n",
    "            pprint.pprint(tag)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '1683602133', 'key': 'name', 'type': None, 'value': 'Village Hall'}]"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'changeset': None,\n",
       " 'id': None,\n",
       " 'lat': None,\n",
       " 'lon': None,\n",
       " 'timestamp': None,\n",
       " 'uid': None,\n",
       " 'user': None,\n",
       " 'version': None}"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pprint \n",
    "element = get_element('example_3.osm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_elems = []\n",
    "for elem in element: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Element 'node' at 0x10b9631b0>,\n",
      " <Element 'node' at 0x10b963330>,\n",
      " <Element 'node' at 0x10b9634b0>,\n",
      " <Element 'node' at 0x10b963630>,\n",
      " <Element 'node' at 0x10b9637b0>,\n",
      " <Element 'node' at 0x10b963930>,\n",
      " <Element 'node' at 0x10b963ab0>,\n",
      " <Element 'node' at 0x10b963c30>,\n",
      " <Element 'node' at 0x10b963db0>,\n",
      " <Element 'node' at 0x10b963f30>,\n",
      " <Element 'node' at 0x10b9f80f0>,\n",
      " <Element 'node' at 0x10b9f8270>,\n",
      " <Element 'node' at 0x10b9f83f0>,\n",
      " <Element 'node' at 0x10b9f8570>,\n",
      " <Element 'node' at 0x10b9f86f0>,\n",
      " <Element 'node' at 0x10b9f8840>,\n",
      " <Element 'node' at 0x10b9f89c0>,\n",
      " <Element 'node' at 0x10b9f8b40>,\n",
      " <Element 'node' at 0x10b9f8d20>,\n",
      " <Element 'node' at 0x10b9fa360>,\n",
      " <Element 'node' at 0x10b9fa900>,\n",
      " <Element 'node' at 0x10b9faa80>,\n",
      " <Element 'way' at 0x10b9fad20>,\n",
      " <Element 'node' at 0x10b9fc090>,\n",
      " <Element 'way' at 0x10b9fc510>,\n",
      " <Element 'relation' at 0x10b9fcbd0>]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(all_elems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
