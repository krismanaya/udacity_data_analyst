{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "## Homework problems for DataWrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is how you manually parse a cvs file in a dictionary.\n",
    "# Your task is to read the input DATAFILE line by line, and for the first 10 lines (not including the header)\n",
    "# split each line on \",\" and then for each line, create a dictionary\n",
    "# where the key is the header title of the field, and the value is the value of that field in the row.\n",
    "# The function parse_file should return a list of dictionaries,\n",
    "# each data line in the file being a single list entry.\n",
    "# Field names and values should not contain extra whitespace, like spaces or newline characters.\n",
    "# You can use the Python string method strip() to remove the extra whitespace.\n",
    "# You have to parse only the first 10 data lines in this exercise,\n",
    "# so the returned list should have 10 entries!\n",
    "\n",
    "def parse_file(datafile): \n",
    "    data = []\n",
    "    with open(datafile,'rb') as f: \n",
    "        header = f.readline().split(',')\n",
    "        counter = 0\n",
    "        for line in f: \n",
    "            if counter == 10: \n",
    "                break \n",
    "            d = {}\n",
    "            fields = line.split(',') \n",
    "            for i,value in enumerate(fields): \n",
    "                d[header[i].strip()] = value.strip()\n",
    "            data.append(d)\n",
    "            counter += 1\n",
    "    return data\n",
    "\n",
    "#Parse File w/o csv module\n",
    "# parse_file('beatles-diskography.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'BPI Certification': 'Gold',\n",
       "  'Label': 'Parlophone(UK)',\n",
       "  'RIAA Certification': 'Platinum',\n",
       "  'Released': '22 March 1963',\n",
       "  'Title': 'Please Please Me',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '\\xe2\\x80\\x94'},\n",
       " {'BPI Certification': 'Platinum',\n",
       "  'Label': 'Parlophone(UK)',\n",
       "  'RIAA Certification': 'Gold',\n",
       "  'Released': '22 November 1963',\n",
       "  'Title': 'With the Beatles',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '\\xe2\\x80\\x94'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(CAN)',\n",
       "  'RIAA Certification': '',\n",
       "  'Released': '25 November 1963',\n",
       "  'Title': 'Beatlemania! With the Beatles',\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '\\xe2\\x80\\x94'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Vee-Jay(US)',\n",
       "  'RIAA Certification': '',\n",
       "  'Released': '10 January 1964',\n",
       "  'Title': 'Introducing... The Beatles',\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '2'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(US)',\n",
       "  'RIAA Certification': '5xPlatinum',\n",
       "  'Released': '20 January 1964',\n",
       "  'Title': 'Meet the Beatles!',\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(CAN)',\n",
       "  'RIAA Certification': '',\n",
       "  'Released': '3 February 1964',\n",
       "  'Title': 'Twist and Shout',\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '\\xe2\\x80\\x94'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(US)',\n",
       "  'RIAA Certification': '2xPlatinum',\n",
       "  'Released': '10 April 1964',\n",
       "  'Title': \"The Beatles' Second Album\",\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(CAN)',\n",
       "  'RIAA Certification': '',\n",
       "  'Released': '11 May 1964',\n",
       "  'Title': \"The Beatles' Long Tall Sally\",\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '\\xe2\\x80\\x94'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'United Artists(US)[C]',\n",
       "  'RIAA Certification': '4xPlatinum',\n",
       "  'Released': '26 June 1964',\n",
       "  'Title': \"A Hard Day's Night\",\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': 'Gold',\n",
       "  'Label': 'Parlophone(UK)',\n",
       "  'RIAA Certification': '',\n",
       "  'Released': '10 July 1964',\n",
       "  'Title': '',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '\\xe2\\x80\\x94'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(US)',\n",
       "  'RIAA Certification': 'Platinum',\n",
       "  'Released': '20 July 1964',\n",
       "  'Title': 'Something New',\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '2'},\n",
       " {'BPI Certification': 'Gold',\n",
       "  'Label': 'Parlophone(UK)',\n",
       "  'RIAA Certification': 'Platinum',\n",
       "  'Released': '4 December 1964',\n",
       "  'Title': 'Beatles for Sale',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '\\xe2\\x80\\x94'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(US)',\n",
       "  'RIAA Certification': '3xPlatinum',\n",
       "  'Released': '15 December 1964',\n",
       "  'Title': \"Beatles '65\",\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Parlophone(NZ), Capitol(US)',\n",
       "  'RIAA Certification': 'Platinum',\n",
       "  'Released': '14 June 1965',\n",
       "  'Title': 'Beatles VI',\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': 'Platinum',\n",
       "  'Label': 'Parlophone(UK)',\n",
       "  'RIAA Certification': '',\n",
       "  'Released': '6 August 1965',\n",
       "  'Title': 'Help!',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '\\xe2\\x80\\x94'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(US)[C]',\n",
       "  'RIAA Certification': '3xPlatinum',\n",
       "  'Released': '13 August 1965',\n",
       "  'Title': '',\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': 'Platinum',\n",
       "  'Label': 'Parlophone(UK)',\n",
       "  'RIAA Certification': '',\n",
       "  'Released': '3 December 1965',\n",
       "  'Title': 'Rubber Soul',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '\\xe2\\x80\\x94'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(US)[C]',\n",
       "  'RIAA Certification': '6xPlatinum',\n",
       "  'Released': '6 December 1965',\n",
       "  'Title': '',\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(US)',\n",
       "  'RIAA Certification': '2xPlatinum',\n",
       "  'Released': '15 June 1966',\n",
       "  'Title': 'Yesterday and Today',\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': 'Platinum',\n",
       "  'Label': 'Parlophone(UK)',\n",
       "  'RIAA Certification': '',\n",
       "  'Released': '5 August 1966',\n",
       "  'Title': 'Revolver',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '\\xe2\\x80\\x94'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(US)[C]',\n",
       "  'RIAA Certification': '5xPlatinum',\n",
       "  'Released': '8 August 1966',\n",
       "  'Title': '',\n",
       "  'UK Chart Position': '\\xe2\\x80\\x94',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': '3xPlatinum',\n",
       "  'Label': 'Parlophone(UK), Capitol(US)',\n",
       "  'RIAA Certification': '11xPlatinum',\n",
       "  'Released': '1 June 1967',\n",
       "  'Title': \"Sgt. Pepper's Lonely Hearts Club Band\",\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': 'Platinum',\n",
       "  'Label': 'Parlophone(UK), Capitol(US)',\n",
       "  'RIAA Certification': '6xPlatinum',\n",
       "  'Released': '27 November 1967',\n",
       "  'Title': 'Magical Mystery Tour',\n",
       "  'UK Chart Position': '31[D]',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': 'Platinum',\n",
       "  'Label': 'Apple(UK), Capitol(US)',\n",
       "  'RIAA Certification': '19xPlatinum',\n",
       "  'Released': '22 November 1968',\n",
       "  'Title': 'The Beatles',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': 'Silver',\n",
       "  'Label': 'Apple(UK), Capitol(US)',\n",
       "  'RIAA Certification': 'Platinum',\n",
       "  'Released': '13 January 1969',\n",
       "  'Title': 'Yellow Submarine',\n",
       "  'UK Chart Position': '3',\n",
       "  'US Chart Position': '2'},\n",
       " {'BPI Certification': '2xPlatinum',\n",
       "  'Label': 'Apple(UK), Capitol(US)',\n",
       "  'RIAA Certification': '12xPlatinum',\n",
       "  'Released': '26 September 1969',\n",
       "  'Title': 'Abbey Road',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': 'Gold',\n",
       "  'Label': 'Apple(UK),United Artists(US)',\n",
       "  'RIAA Certification': '4xPlatinum',\n",
       "  'Released': '8 May 1970',\n",
       "  'Title': 'Let It Be',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '1'}]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now parse the data using DictReader csv module\n",
    "import csv\n",
    "def parse_csv_dr(datafile): \n",
    "    data = []\n",
    "    n = 0 \n",
    "    with open(datafile,'rb') as sd: \n",
    "        r = csv.DictReader(sd)\n",
    "        for line in r: \n",
    "            data.append(line)\n",
    "    return data\n",
    "\n",
    "parse_csv_dr('beatles-diskography.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Intro to the module XLRD for Excel files. \n",
    "import xlrd \n",
    "\n",
    "#import the file\n",
    "datafile = '2013_ERCOT_Hourly_Load_Data.xls'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List Comprehension\n",
      "data[3][2]: 1036.2167394\n",
      "\n",
      "Cells in a nested loop:\n",
      "41277.0833333 9240.30996952 1438.44928594 1565.62980305 894.858053634 14013.232226 3028.50276713 6165.28163968 1157.91411201 37504.177857 \n",
      "ROWS, COLUMNS and CELLS:\n",
      "Number of rows in the sheet: 8761\n",
      "Type of data in cell (row 3, col 2): 2\n",
      "Value in cell (row 3, col 2): 1036.2167394\n",
      "Get a slice of values in column 3, from rows 1-3:\n",
      "[1411.823158897756, 1403.6010926983663, 1395.1312126407752]\n",
      "\n",
      "DATES:\n",
      "Tytpe of data in cell (row 1, col 0): 3\n",
      "Time in Excel format: 41275.0416667\n",
      "Convert time to a Python datetime tuple, from the Excel float: (2013, 1, 1, 1, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# create a parse_file function \n",
    "def parse_file(datafile): \n",
    "    workbook = xlrd.open_workbook(datafile) # open the excel file \n",
    "    sheet = workbook.sheet_by_index(0) # indicate what sheet we use \n",
    "    \n",
    "    data = [[sheet.cell_value(r,col)\n",
    "                for col in range(sheet.ncols)]\n",
    "                   for r in range(sheet.nrows)] # iterating through the sheet into a list \n",
    "    \n",
    "    print \"\\nList Comprehension\"\n",
    "    print \"data[3][2]:\", \n",
    "    print data[3][2]\n",
    "    \n",
    "    print \"\\nCells in a nested loop:\"\n",
    "    for row in range(sheet.nrows): \n",
    "        for col in range(sheet.ncols): \n",
    "            if row == 50: \n",
    "                print sheet.cell_value(row, col), \n",
    "                \n",
    "             \n",
    "    ### other useful methods: \n",
    "    print \"\\nROWS, COLUMNS and CELLS:\"\n",
    "    print \"Number of rows in the sheet:\",\n",
    "    print sheet.nrows\n",
    "    print \"Type of data in cell (row 3, col 2):\", \n",
    "    print sheet.cell_type(3,2)\n",
    "    print \"Value in cell (row 3, col 2):\", \n",
    "    print sheet.cell_value(3,2)\n",
    "    print \"Get a slice of values in column 3, from rows 1-3:\"\n",
    "    print sheet.col_values(3,start_rowx=1,end_rowx=4)\n",
    "    \n",
    "    print \"\\nDATES:\"\n",
    "    print \"Tytpe of data in cell (row 1, col 0):\", \n",
    "    print sheet.cell_type(1,0)\n",
    "    exceltime = sheet.cell_value(1,0)\n",
    "    print \"Time in Excel format:\", \n",
    "    print exceltime\n",
    "    print \"Convert time to a Python datetime tuple, from the Excel float:\",\n",
    "    print xlrd.xldate_as_tuple(exceltime, 0)\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = parse_file(datafile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting http://musicbrainz.org/ws/2/artist/?query=artist%3ANirvana&fmt=json\n",
      "{\n",
      "    \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
      "    \"name\": \"United Kingdom\", \n",
      "    \"sort-name\": \"United Kingdom\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# To experiment with this code freely you will have to run this code locally.\n",
    "# Take a look at the main() function for an example of how to use the code.\n",
    "# We have provided example json output in the other code editor tabs for you to\n",
    "# look at, but you will not be able to run any queries through our UI.\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "BASE_URL = \"http://musicbrainz.org/ws/2/\"\n",
    "ARTIST_URL = BASE_URL + \"artist/\"\n",
    "\n",
    "# query parameters are given to the requests.get function as a dictionary; this\n",
    "# variable contains some starter parameters.\n",
    "query_type = {  \"simple\": {},\n",
    "                \"atr\": {\"inc\": \"aliases+tags+ratings\"},\n",
    "                \"aliases\": {\"inc\": \"aliases\"},\n",
    "                \"releases\": {\"inc\": \"releases\"}}\n",
    "\n",
    "\n",
    "def query_site(url, params, uid=\"\", fmt=\"json\"):\n",
    "    # This is the main function for making queries to the musicbrainz API.\n",
    "    # A json document should be returned by the query.\n",
    "    params[\"fmt\"] = fmt\n",
    "    r = requests.get(url + uid, params=params)\n",
    "    print \"requesting\", r.url\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def query_by_name(url, params, name):\n",
    "    # This adds an artist name to the query parameters before making\n",
    "    # an API call to the function above.\n",
    "    params[\"query\"] = \"artist:\" + name\n",
    "    return query_site(url, params)\n",
    "\n",
    "\n",
    "def pretty_print(data, indent=4):\n",
    "    # After we get our output, we can format it to be more readable\n",
    "    # by using this function.\n",
    "    if type(data) == dict:\n",
    "        print json.dumps(data, indent=indent, sort_keys=True)\n",
    "    else:\n",
    "        print data\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    Modify the function calls and indexing below to answer the questions on\n",
    "    the next quiz. HINT: Note how the output we get from the site is a\n",
    "    multi-level JSON document, so try making print statements to step through\n",
    "    the structure one level at a time or copy the output to a separate output\n",
    "    file.\n",
    "    '''\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"Nirvana\")\n",
    "    pretty_print(results['artists'][0]['area'])\n",
    "\n",
    "#     artist_id = results[\"artists\"][0]['id']\n",
    "#     print \"\\nARTIST:\"\n",
    "#     pretty_print(results[\"artists\"][0])\n",
    "\n",
    "#     artist_data = query_site(ARTIST_URL, query_type[\"releases\"], artist_id)\n",
    "#     pretty_print(artist_data) \n",
    "#     releases = artist_data[\"releases\"]\n",
    "#     print \"\\nONE RELEASE:\"\n",
    "#     pretty_print(releases[0], indent=2)\n",
    "#     release_titles = [r[\"title\"] for r in releases]\n",
    "\n",
    "#     print \"\\nALL TITLES:\"\n",
    "#     for t in release_titles:\n",
    "#         print t\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Children of root:\n",
      "ui\n",
      "ji\n",
      "fm\n",
      "bdy\n",
      "bm\n"
     ]
    }
   ],
   "source": [
    "## XML training. \n",
    "import xml.etree.ElementTree as ET \n",
    "import pprint \n",
    "\n",
    "# XML: parse the file \n",
    "tree = ET.parse('exampleResearchArticle.xml')\n",
    "\n",
    "# Get its root \n",
    "root = tree.getroot() \n",
    "\n",
    "# print the object \n",
    "print \"\\nChildren of root:\"\n",
    "for child in root: \n",
    "    print child.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title:\n",
      "Standardization of the functional syndesmosis widening by dynamic U.S examination\n"
     ]
    }
   ],
   "source": [
    "# extract from fm to find its children and find its title\n",
    "title = root.find(\"./fm/bibl/title\")\n",
    "title_text = \"\"\n",
    "for p in title: \n",
    "    title_text += p.text\n",
    "print \"\\nTitle:\\n\", title_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Author email addresses:\n",
      "omer@extremegate.com\n",
      "mcarmont@hotmail.com\n",
      "laver17@gmail.com\n",
      "nyska@internet-zahav.net\n",
      "kammarh@gmail.com\n",
      "gideon.mann.md@gmail.com\n",
      "barns.nz@gmail.com\n",
      "eukots@gmail.com\n"
     ]
    }
   ],
   "source": [
    "# find the author email addresses\n",
    "print \"\\nAuthor email addresses:\"\n",
    "for a in root.findall(\"./fm/bibl/aug/au\"): \n",
    "    email = a.find('email')\n",
    "    if email is not None: \n",
    "        print email.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# authors = []\n",
    "# for author in root.findall(\"./fm/bibl/aug/au\"): \n",
    "#     data = {'fnm': None, \"snm\": None, \"email\": None, 'insr': []}\n",
    "#     data['fnm'] = author.find('./fnm').text\n",
    "#     data['snm'] = author.find('./snm').text\n",
    "#     data['email'] = author.find('./email').text\n",
    "#     insr = author.findall(\"./insr\")\n",
    "#     for i in insr: \n",
    "#         data['insr'].append(i.attrib['iid'])\n",
    "#     auother.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carriers:\n",
      "All\n",
      "AllUS\n",
      "AllForeign\n",
      "AS\n",
      "G4\n",
      "AA\n",
      "5Y\n",
      "DL\n",
      "MQ\n",
      "EV\n",
      "F9\n",
      "HA\n",
      "B6\n",
      "OO\n",
      "WN\n",
      "NK\n",
      "UA\n",
      "VX\n",
      "\n",
      "Airports:\n",
      "All\n",
      "AllMajors\n",
      "ATL\n",
      "BWI\n",
      "BOS\n",
      "CLT\n",
      "MDW\n",
      "ORD\n",
      "DAL\n",
      "DFW\n",
      "DEN\n",
      "DTW\n",
      "FLL\n",
      "IAH\n",
      "LAS\n",
      "LAX\n",
      "MIA\n",
      "MSP\n",
      "JFK\n",
      "LGA\n",
      "EWR\n",
      "MCO\n",
      "PHL\n",
      "PHX\n",
      "PDX\n",
      "SLC\n",
      "SAN\n",
      "SFO\n",
      "SEA\n",
      "TPA\n",
      "DCA\n",
      "IAD\n",
      "AllOthers\n",
      "UXM\n",
      "ABR\n",
      "ABI\n",
      "DYS\n",
      "ADK\n",
      "VZF\n",
      "BQN\n",
      "AKK\n",
      "KKI\n",
      "AKI\n",
      "AKO\n",
      "CAK\n",
      "7AK\n",
      "KQA\n",
      "AUK\n",
      "ALM\n",
      "ALS\n",
      "ABY\n",
      "ALB\n",
      "ABQ\n",
      "ZXB\n",
      "WKK\n",
      "AED\n",
      "AEX\n",
      "AXN\n",
      "AET\n",
      "ABE\n",
      "AIA\n",
      "APN\n",
      "DQH\n",
      "AOO\n",
      "AMA\n",
      "ABL\n",
      "OQZ\n",
      "AOS\n",
      "OTS\n",
      "AKP\n",
      "EDF\n",
      "DQL\n",
      "MRI\n",
      "ANC\n",
      "AND\n",
      "AGN\n",
      "ANI\n",
      "ANN\n",
      "ANB\n",
      "ANV\n",
      "ATW\n",
      "ACV\n",
      "ARC\n",
      "ADM\n",
      "AVL\n",
      "HTS\n",
      "ASE\n",
      "AST\n",
      "AHN\n",
      "AKB\n",
      "PDK\n",
      "FTY\n",
      "ACY\n",
      "ATT\n",
      "ATK\n",
      "MER\n",
      "AUO\n",
      "AGS\n",
      "AUG\n",
      "AUS\n",
      "A28\n",
      "BFL\n",
      "BGR\n",
      "BHB\n",
      "BRW\n",
      "BTI\n",
      "BQV\n",
      "A2K\n",
      "BTR\n",
      "BTL\n",
      "AK2\n",
      "A56\n",
      "BTY\n",
      "BPT\n",
      "BVD\n",
      "WBQ\n",
      "BKW\n",
      "BED\n",
      "A11\n",
      "KBE\n",
      "BLV\n",
      "BLI\n",
      "BLM\n",
      "JVL\n",
      "BVU\n",
      "BJI\n",
      "RDM\n",
      "BEH\n",
      "BET\n",
      "BTT\n",
      "BVY\n",
      "OQB\n",
      "A50\n",
      "BIC\n",
      "BIG\n",
      "BGQ\n",
      "BMX\n",
      "PWR\n",
      "A85\n",
      "BIL\n",
      "BIX\n",
      "BGM\n",
      "KBC\n",
      "BHM\n",
      "BIS\n",
      "BYW\n",
      "BID\n",
      "BMG\n",
      "BMI\n",
      "BFB\n",
      "BYH\n",
      "BCT\n",
      "BOI\n",
      "RLU\n",
      "BXS\n",
      "BLD\n",
      "BYA\n",
      "BWG\n",
      "BZN\n",
      "BFD\n",
      "A23\n",
      "BRD\n",
      "BKG\n",
      "PWT\n",
      "KTS\n",
      "BDR\n",
      "TRI\n",
      "BKX\n",
      "RBH\n",
      "BRO\n",
      "BWD\n",
      "BQK\n",
      "BCE\n",
      "BKC\n",
      "BUF\n",
      "IFP\n",
      "BUR\n",
      "BRL\n",
      "BTV\n",
      "MVW\n",
      "BNO\n",
      "BTM\n",
      "JQF\n",
      "UXI\n",
      "CDW\n",
      "C01\n",
      "ADW\n",
      "CDL\n",
      "CGI\n",
      "LUR\n",
      "EHM\n",
      "CZF\n",
      "A61\n",
      "A40\n",
      "CYT\n",
      "MDH\n",
      "CLD\n",
      "CNM\n",
      "A87\n",
      "CPR\n",
      "CDC\n",
      "CID\n",
      "JRV\n",
      "NRR\n",
      "CEM\n",
      "CDR\n",
      "CIK\n",
      "CMI\n",
      "WCR\n",
      "CHS\n",
      "CRW\n",
      "SPB\n",
      "STT\n",
      "CHO\n",
      "CYM\n",
      "CHA\n",
      "CYF\n",
      "WA7\n",
      "CEX\n",
      "EGA\n",
      "NCN\n",
      "KCN\n",
      "VAK\n",
      "CYS\n",
      "PWK\n",
      "CHI\n",
      "DPA\n",
      "LOT\n",
      "CKX\n",
      "CIC\n",
      "CEF\n",
      "KCG\n",
      "KCL\n",
      "WQZ\n",
      "KCQ\n",
      "CZN\n",
      "CIV\n",
      "ZXH\n",
      "SSB\n",
      "STX\n",
      "CHU\n",
      "LUK\n",
      "CVG\n",
      "OQC\n",
      "A12\n",
      "CHP\n",
      "IRC\n",
      "CLP\n",
      "CKB\n",
      "BKL\n",
      "CLE\n",
      "CGF\n",
      "CFT\n",
      "CLK\n",
      "ZXN\n",
      "CVN\n",
      "ZXI\n",
      "OOB\n",
      "COD\n",
      "CFA\n",
      "KCC\n",
      "A69\n",
      "CDB\n",
      "CXF\n",
      "CLL\n",
      "KCR\n",
      "COS\n",
      "COA\n",
      "COU\n",
      "CAE\n",
      "CSG\n",
      "CBM\n",
      "GTR\n",
      "OSU\n",
      "CMH\n",
      "LCK\n",
      "CCR\n",
      "CKU\n",
      "CDV\n",
      "CBA\n",
      "CRP\n",
      "CEZ\n",
      "CVO\n",
      "CIL\n",
      "CGA\n",
      "CEC\n",
      "CKD\n",
      "CUW\n",
      "CPX\n",
      "CBE\n",
      "DCK\n",
      "ADS\n",
      "RBD\n",
      "AFW\n",
      "FTW\n",
      "DGB\n",
      "DAN\n",
      "WQW\n",
      "MGY\n",
      "DAY\n",
      "DAB\n",
      "AA8\n",
      "SCC\n",
      "FVZ\n",
      "A02\n",
      "DTH\n",
      "DTR\n",
      "DEC\n",
      "XXV\n",
      "A36\n",
      "DHB\n",
      "DRG\n",
      "DRT\n",
      "DLF\n",
      "DJN\n",
      "DMN\n",
      "DTO\n",
      "APA\n",
      "FTG\n",
      "DSM\n",
      "DSI\n",
      "DTL\n",
      "DET\n",
      "DTT\n",
      "YIP\n",
      "DVL\n",
      "DIK\n",
      "DLG\n",
      "DIO\n",
      "DDC\n",
      "FVQ\n",
      "DOF\n",
      "DHN\n",
      "DOV\n",
      "DRF\n",
      "A22\n",
      "FQQ\n",
      "DUJ\n",
      "DBQ\n",
      "DLH\n",
      "A4K\n",
      "AMK\n",
      "DRO\n",
      "EAA\n",
      "EGE\n",
      "FRG\n",
      "HTO\n",
      "GA0\n",
      "ESN\n",
      "ESD\n",
      "EAU\n",
      "EDA\n",
      "EDW\n",
      "EEK\n",
      "EGX\n",
      "KKU\n",
      "KEK\n",
      "ZXO\n",
      "IPL\n",
      "ELD\n",
      "BIF\n",
      "ELP\n",
      "ELV\n",
      "ELI\n",
      "EKO\n",
      "ELM\n",
      "LYU\n",
      "ELY\n",
      "EMK\n",
      "WDG\n",
      "ERI\n",
      "ESC\n",
      "EUG\n",
      "EVV\n",
      "EVM\n",
      "PAE\n",
      "EXI\n",
      "EIL\n",
      "FAI\n",
      "FBK\n",
      "A01\n",
      "A6K\n",
      "SUU\n",
      "FAJ\n",
      "KFP\n",
      "FWL\n",
      "FAR\n",
      "FMN\n",
      "FYV\n",
      "XNA\n",
      "FAY\n",
      "POB\n",
      "FFM\n",
      "FIC\n",
      "FAQ\n",
      "FLG\n",
      "FNT\n",
      "FLO\n",
      "FNL\n",
      "WRI\n",
      "FOD\n",
      "FQW\n",
      "FHU\n",
      "TBN\n",
      "RSW\n",
      "FPR\n",
      "FSI\n",
      "FSM\n",
      "FWA\n",
      "FWH\n",
      "FYU\n",
      "FKL\n",
      "VZE\n",
      "FAT\n",
      "FRD\n",
      "FBS\n",
      "FNR\n",
      "GNV\n",
      "GVL\n",
      "GBH\n",
      "GAL\n",
      "GUP\n",
      "GAM\n",
      "GEK\n",
      "GCK\n",
      "GYY\n",
      "GCC\n",
      "AQY\n",
      "GGW\n",
      "AZ3\n",
      "GDV\n",
      "AK6\n",
      "FVW\n",
      "GLV\n",
      "GNU\n",
      "GYR\n",
      "FVX\n",
      "JGC\n",
      "GCN\n",
      "AZ1\n",
      "GFK\n",
      "GRI\n",
      "GJT\n",
      "GRR\n",
      "GPZ\n",
      "VWZ\n",
      "GMT\n",
      "XWA\n",
      "KGX\n",
      "GBD\n",
      "GTF\n",
      "GRB\n",
      "GSO\n",
      "GLH\n",
      "PGV\n",
      "GVT\n",
      "GSP\n",
      "UAM\n",
      "GUM\n",
      "GUF\n",
      "GPT\n",
      "GKN\n",
      "GUC\n",
      "GST\n",
      "HGR\n",
      "HNS\n",
      "A03\n",
      "HNM\n",
      "CMX\n",
      "VWD\n",
      "ZXJ\n",
      "HRL\n",
      "MDT\n",
      "HRO\n",
      "BDL\n",
      "PIB\n",
      "HVR\n",
      "HWI\n",
      "HHR\n",
      "HDN\n",
      "HYS\n",
      "HKB\n",
      "HLN\n",
      "T2X\n",
      "HES\n",
      "HIB\n",
      "HKY\n",
      "HIO\n",
      "ITO\n",
      "HHH\n",
      "HBH\n",
      "HOB\n",
      "HGZ\n",
      "HOL\n",
      "HYL\n",
      "HCR\n",
      "HOM\n",
      "HST\n",
      "VWX\n",
      "HNL\n",
      "MKK\n",
      "HNH\n",
      "HPB\n",
      "HOP\n",
      "HOT\n",
      "DWH\n",
      "EFD\n",
      "HOU\n",
      "HUS\n",
      "HSV\n",
      "HON\n",
      "HSL\n",
      "HUT\n",
      "HYA\n",
      "HYG\n",
      "WHD\n",
      "ICY\n",
      "IDA\n",
      "IGG\n",
      "ILI\n",
      "ZXF\n",
      "IND\n",
      "MQJ\n",
      "INL\n",
      "A57\n",
      "IYK\n",
      "IMT\n",
      "IWD\n",
      "ISP\n",
      "SAW\n",
      "ITH\n",
      "KIB\n",
      "A59\n",
      "A26\n",
      "MKL\n",
      "JAC\n",
      "JAN\n",
      "NZC\n",
      "JAX\n",
      "NIP\n",
      "OAJ\n",
      "JMS\n",
      "JHW\n",
      "VZM\n",
      "JON\n",
      "JST\n",
      "JBR\n",
      "JLN\n",
      "JNU\n",
      "OGG\n",
      "KAE\n",
      "A37\n",
      "A35\n",
      "KKK\n",
      "AZO\n",
      "LUP\n",
      "FCA\n",
      "KLG\n",
      "KAL\n",
      "MUE\n",
      "KNB\n",
      "MKC\n",
      "MCI\n",
      "JHM\n",
      "JRF\n",
      "KKL\n",
      "A65\n",
      "KYK\n",
      "KXA\n",
      "KUK\n",
      "VZR\n",
      "VZY\n",
      "FQD\n",
      "VIK\n",
      "MVM\n",
      "EAR\n",
      "EEN\n",
      "ENA\n",
      "KEH\n",
      "KTN\n",
      "WFB\n",
      "DQU\n",
      "EYW\n",
      "NQX\n",
      "QQB\n",
      "IAN\n",
      "GRK\n",
      "ILE\n",
      "A29\n",
      "KVC\n",
      "AKN\n",
      "IGM\n",
      "ISO\n",
      "KPN\n",
      "IRK\n",
      "KKB\n",
      "KVL\n",
      "KZH\n",
      "06A\n",
      "LMT\n",
      "KLW\n",
      "SZL\n",
      "TYS\n",
      "OBU\n",
      "A43\n",
      "ADQ\n",
      "KDK\n",
      "A41\n",
      "KNK\n",
      "KGK\n",
      "KOA\n",
      "KKH\n",
      "KOT\n",
      "OTZ\n",
      "KKA\n",
      "KYU\n",
      "LKK\n",
      "UUK\n",
      "KWT\n",
      "KWK\n",
      "LSE\n",
      "LAF\n",
      "LFT\n",
      "LCH\n",
      "XXW\n",
      "HII\n",
      "LMA\n",
      "TVL\n",
      "LNY\n",
      "ZXK\n",
      "WJF\n",
      "LNS\n",
      "LAN\n",
      "LAR\n",
      "LRD\n",
      "KLN\n",
      "HSH\n",
      "LSV\n",
      "VGT\n",
      "LBE\n",
      "LZU\n",
      "LAW\n",
      "ALZ\n",
      "LEB\n",
      "VA4\n",
      "KLL\n",
      "LWB\n",
      "LWS\n",
      "LEW\n",
      "LWT\n",
      "LEX\n",
      "LBL\n",
      "LIH\n",
      "UXA\n",
      "LVD\n",
      "LNK\n",
      "LIT\n",
      "05A\n",
      "LGU\n",
      "LNI\n",
      "LGB\n",
      "LIJ\n",
      "GGG\n",
      "LPS\n",
      "LPR\n",
      "LAM\n",
      "SDF\n",
      "LBB\n",
      "LYH\n",
      "MCN\n",
      "MSN\n",
      "A75\n",
      "MMH\n",
      "MNZ\n",
      "MHT\n",
      "MHK\n",
      "MBL\n",
      "MLY\n",
      "KMO\n",
      "MZJ\n",
      "MTH\n",
      "MYH\n",
      "MWA\n",
      "MQT\n",
      "MLL\n",
      "MVY\n",
      "MCW\n",
      "MSS\n",
      "MYK\n",
      "MAZ\n",
      "MYL\n",
      "MXY\n",
      "MCK\n",
      "OQA\n",
      "MCG\n",
      "MCL\n",
      "MFR\n",
      "MDR\n",
      "MYU\n",
      "MLB\n",
      "OQL\n",
      "MEM\n",
      "XXX\n",
      "MCE\n",
      "MEI\n",
      "OQM\n",
      "MFH\n",
      "MTM\n",
      "WMK\n",
      "MPB\n",
      "6B0\n",
      "MDO\n",
      "MAF\n",
      "MDY\n",
      "MLS\n",
      "NQA\n",
      "MKE\n",
      "MHM\n",
      "MWL\n",
      "STP\n",
      "MIB\n",
      "MOT\n",
      "MNT\n",
      "MFE\n",
      "MSO\n",
      "CNY\n",
      "BFM\n",
      "MOB\n",
      "MOD\n",
      "VZG\n",
      "MLI\n",
      "MLU\n",
      "MRY\n",
      "MGM\n",
      "MTJ\n",
      "UXR\n",
      "MGW\n",
      "MMU\n",
      "MVL\n",
      "KMY\n",
      "MWH\n",
      "MOS\n",
      "CWA\n",
      "MUO\n",
      "NUQ\n",
      "MOU\n",
      "A13\n",
      "MSL\n",
      "VZC\n",
      "MKG\n",
      "MYR\n",
      "NNK\n",
      "WQR\n",
      "AA2\n",
      "ACK\n",
      "KEB\n",
      "APC\n",
      "WNA\n",
      "KPM\n",
      "PKA\n",
      "APF\n",
      "BNA\n",
      "NKI\n",
      "NLG\n",
      "ENN\n",
      "EWB\n",
      "EWN\n",
      "HVN\n",
      "ARA\n",
      "GON\n",
      "NEW\n",
      "MSY\n",
      "DQN\n",
      "KNW\n",
      "JRB\n",
      "TSS\n",
      "NYC\n",
      "SWF\n",
      "LFI\n",
      "PHF\n",
      "ONP\n",
      "WWT\n",
      "EWK\n",
      "IAG\n",
      "NME\n",
      "NIB\n",
      "IKO\n",
      "NIN\n",
      "RQI\n",
      "WTK\n",
      "OME\n",
      "NNL\n",
      "ORV\n",
      "OFK\n",
      "ORF\n",
      "NGU\n",
      "OTH\n",
      "LBF\n",
      "MA5\n",
      "OHC\n",
      "ORT\n",
      "NUI\n",
      "NUL\n",
      "NUP\n",
      "ZNC\n",
      "ODW\n",
      "OAK\n",
      "OCF\n",
      "OFU\n",
      "HIF\n",
      "OGD\n",
      "OGS\n",
      "OKC\n",
      "OJC\n",
      "JCI\n",
      "OLH\n",
      "KOY\n",
      "XWS\n",
      "OLV\n",
      "OLM\n",
      "OMA\n",
      "ONN\n",
      "ONT\n",
      "OPH\n",
      "ORL\n",
      "OSH\n",
      "KOZ\n",
      "OWB\n",
      "UOX\n",
      "OXR\n",
      "PBK\n",
      "PAH\n",
      "PGA\n",
      "PPG\n",
      "PCE\n",
      "PSP\n",
      "PMD\n",
      "PAQ\n",
      "PFN\n",
      "ECP\n",
      "PAM\n",
      "PKD\n",
      "PKB\n",
      "PSC\n",
      "PRB\n",
      "DQR\n",
      "1G4\n",
      "DQW\n",
      "WQJ\n",
      "PDB\n",
      "PEC\n",
      "PLN\n",
      "PDT\n",
      "PNS\n",
      "NPA\n",
      "PIA\n",
      "KPV\n",
      "VYS\n",
      "GUS\n",
      "PSG\n",
      "PNF\n",
      "PNE\n",
      "LUF\n",
      "DVT\n",
      "AZA\n",
      "SCF\n",
      "PIR\n",
      "PIP\n",
      "UGB\n",
      "PQS\n",
      "SOP\n",
      "AGC\n",
      "PIT\n",
      "PSF\n",
      "PTU\n",
      "PLB\n",
      "PBG\n",
      "PTR\n",
      "PIH\n",
      "A27\n",
      "KPB\n",
      "PHO\n",
      "PIZ\n",
      "POQ\n",
      "PNC\n",
      "PSE\n",
      "PTK\n",
      "PVY\n",
      "PTD\n",
      "PTC\n",
      "PTA\n",
      "CLM\n",
      "KPY\n",
      "KPC\n",
      "PGM\n",
      "PTH\n",
      "A48\n",
      "ORI\n",
      "PML\n",
      "PPV\n",
      "TWD\n",
      "A17\n",
      "KPR\n",
      "PCA\n",
      "WQU\n",
      "PTV\n",
      "PWM\n",
      "PSM\n",
      "PRC\n",
      "PQI\n",
      "PUC\n",
      "BLF\n",
      "PPC\n",
      "PVD\n",
      "PVC\n",
      "PVU\n",
      "PUO\n",
      "A39\n",
      "PUB\n",
      "PUW\n",
      "OQP\n",
      "PGD\n",
      "AK5\n",
      "UIN\n",
      "KWN\n",
      "RDU\n",
      "RMP\n",
      "RCA\n",
      "RAP\n",
      "RDG\n",
      "RDV\n",
      "RDB\n",
      "A76\n",
      "A04\n",
      "RDR\n",
      "RDD\n",
      "RNO\n",
      "RNT\n",
      "RHI\n",
      "RIC\n",
      "RIL\n",
      "RIV\n",
      "RIW\n",
      "ROA\n",
      "RCE\n",
      "RST\n",
      "ROC\n",
      "RKS\n",
      "RFD\n",
      "RKD\n",
      "RWI\n",
      "ROG\n",
      "FAL\n",
      "RME\n",
      "RSJ\n",
      "ROW\n",
      "ROP\n",
      "RBY\n",
      "RUI\n",
      "RSH\n",
      "RSN\n",
      "RUT\n",
      "SAC\n",
      "SMF\n",
      "SAD\n",
      "MBS\n",
      "SPN\n",
      "SLE\n",
      "SLT\n",
      "SLN\n",
      "SNS\n",
      "SBY\n",
      "SMN\n",
      "ZXM\n",
      "SJT\n",
      "SKF\n",
      "SAT\n",
      "NKX\n",
      "MYF\n",
      "NZY\n",
      "SJC\n",
      "WSJ\n",
      "SIG\n",
      "SJU\n",
      "SBP\n",
      "SDP\n",
      "KSR\n",
      "OQS\n",
      "SFB\n",
      "SNA\n",
      "SBA\n",
      "SAF\n",
      "SMX\n",
      "STS\n",
      "SLK\n",
      "SRQ\n",
      "CIU\n",
      "SVN\n",
      "SAV\n",
      "SVA\n",
      "SCM\n",
      "BFF\n",
      "AVP\n",
      "SYB\n",
      "BFI\n",
      "LKE\n",
      "SDX\n",
      "A07\n",
      "WLK\n",
      "SOV\n",
      "A31\n",
      "SQV\n",
      "SWD\n",
      "SHX\n",
      "SKK\n",
      "A90\n",
      "A77\n",
      "SXP\n",
      "SYA\n",
      "SHR\n",
      "OQV\n",
      "SHH\n",
      "SOW\n",
      "BAD\n",
      "SHV\n",
      "SHG\n",
      "SDY\n",
      "SVC\n",
      "SUX\n",
      "FSD\n",
      "NKT\n",
      "SIT\n",
      "SKJ\n",
      "SGY\n",
      "SKW\n",
      "SLQ\n",
      "SCJ\n",
      "MQY\n",
      "SXQ\n",
      "SBN\n",
      "WSN\n",
      "SVW\n",
      "GEG\n",
      "SPI\n",
      "SGF\n",
      "UST\n",
      "STC\n",
      "STG\n",
      "SGU\n",
      "STJ\n",
      "CPS\n",
      "STL\n",
      "SUS\n",
      "KSM\n",
      "SMK\n",
      "SNP\n",
      "PIE\n",
      "RMN\n",
      "STF\n",
      "SCE\n",
      "SHD\n",
      "WSB\n",
      "WBB\n",
      "WA6\n",
      "VZO\n",
      "SVS\n",
      "SWO\n",
      "SCK\n",
      "SRV\n",
      "SSC\n",
      "SUN\n",
      "SYR\n",
      "TCM\n",
      "TIW\n",
      "TCT\n",
      "TKA\n",
      "TLH\n",
      "MCF\n",
      "TAL\n",
      "TSM\n",
      "TLJ\n",
      "TEK\n",
      "TAV\n",
      "TWE\n",
      "TLF\n",
      "TLA\n",
      "TEX\n",
      "TKE\n",
      "HUF\n",
      "A30\n",
      "TEB\n",
      "TEH\n",
      "TXK\n",
      "DLS\n",
      "TVF\n",
      "KTB\n",
      "TNC\n",
      "TIQ\n",
      "TOG\n",
      "TKJ\n",
      "TKI\n",
      "OOK\n",
      "TOL\n",
      "TPH\n",
      "FOE\n",
      "JZE\n",
      "TVC\n",
      "TTN\n",
      "TTD\n",
      "TUS\n",
      "TUL\n",
      "TLT\n",
      "UTM\n",
      "WTL\n",
      "TNK\n",
      "TUP\n",
      "TCL\n",
      "TWF\n",
      "TWA\n",
      "TYR\n",
      "TYE\n",
      "UGI\n",
      "UGS\n",
      "UMT\n",
      "UMB\n",
      "UNK\n",
      "DUT\n",
      "UTO\n",
      "VDZ\n",
      "VLD\n",
      "VPS\n",
      "VPZ\n",
      "VNY\n",
      "VUO\n",
      "VEE\n",
      "VEL\n",
      "VRB\n",
      "VCT\n",
      "VCV\n",
      "A67\n",
      "VQS\n",
      "VCB\n",
      "A70\n",
      "VIS\n",
      "OQI\n",
      "CNW\n",
      "ACT\n",
      "AIN\n",
      "AWK\n",
      "WAA\n",
      "ALW\n",
      "WWA\n",
      "OXC\n",
      "KWF\n",
      "ALO\n",
      "ART\n",
      "ATY\n",
      "EAT\n",
      "ENV\n",
      "VT1\n",
      "AWM\n",
      "PBI\n",
      "KWP\n",
      "WYS\n",
      "WST\n",
      "BAF\n",
      "FOK\n",
      "WSX\n",
      "WWP\n",
      "WMO\n",
      "HPN\n",
      "DQS\n",
      "SPS\n",
      "ICT\n",
      "WDB\n",
      "VZN\n",
      "IPT\n",
      "ISN\n",
      "WOW\n",
      "ILG\n",
      "ILM\n",
      "ILN\n",
      "WGO\n",
      "INW\n",
      "INT\n",
      "WA5\n",
      "WSM\n",
      "OLF\n",
      "ORH\n",
      "WRL\n",
      "WRG\n",
      "YKM\n",
      "YAK\n",
      "XWC\n",
      "WYB\n",
      "YNG\n",
      "A63\n",
      "NYL\n",
      "YUM\n",
      "KZB\n",
      "AK8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 162 of the file /usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# Data Wrangling Procedure \n",
    "# (For this example)\n",
    "# - Build a list of carrier values\n",
    "# - Build a list of airport values \n",
    "# - Make http request to download all data \n",
    "# - Then parse the data files \n",
    "\n",
    "# Best Practices for web scraping: \n",
    "# 1. Look at how a browser makes request\n",
    "# 2. emulate in code \n",
    "# 3. if stuff blows up, look at your Http traffic \n",
    "# 4. Return to 1 until it works. \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from lxml import etree\n",
    "\n",
    "def options(soup, id): \n",
    "    option_values = []\n",
    "    carrier_list = soup.find(id=id)\n",
    "    for option in carrier_list.find_all('option'): \n",
    "        option_values.append(option['value'])\n",
    "    return option_values \n",
    "\n",
    "def print_list(label,codes): \n",
    "    print \"\\n%s:\" % label\n",
    "    for c in codes: \n",
    "        print c \n",
    "\n",
    "def main(): \n",
    "    soup = BeautifulSoup(open(\"virgin_and_logan_airport.html\")) #Sacrament SouthWest Flights\n",
    "    codes = options(soup,'CarrierList')\n",
    "    print_list('Carriers',codes)\n",
    "    \n",
    "    codes = options(soup,'AirportList')\n",
    "    print_list('Airports', codes)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scraping a solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "html_page = \"virgin_and_logan_airport.html\"\n",
    "\n",
    "def extract_data(page): \n",
    "    data = {'eventvalidation': \"\", \n",
    "            \"viewstate\": \"\"}\n",
    "    \n",
    "    with open(page,'r') as html: \n",
    "        soup = BeautifulSoup(html,'lxml')\n",
    "        ev = soup.find(id='__EVENTVALIDATION')\n",
    "        data['eventvalidation'] = ev['value']\n",
    "        \n",
    "        vs = soup.find(id='__VIEWSTATE')\n",
    "        data['viewstate'] = vs['value']\n",
    "        \n",
    "    return data\n",
    "\n",
    "def make_request(data): \n",
    "    eventvalidation = data['eventvalidation']\n",
    "    viewstate = data['viewstate']\n",
    "    r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                    data={'AirportList': \"BOS\",\n",
    "                          'CarrierList': \"VX\",\n",
    "                          'Submit': 'Submit',\n",
    "                          \"__EVENTTARGET\": \"\",\n",
    "                          \"__EVENTARGUMENT\": \"\",\n",
    "                          \"__EVENTVALIDATION\": eventvalidation,\n",
    "                          \"__VIEWSTATE\": viewstate\n",
    "                    })\n",
    "\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    data = extract_data(html_page)\n",
    "    assert data[\"eventvalidation\"] != \"\"\n",
    "    assert data[\"eventvalidation\"].startswith(\"/wEWjAkCoIj1ng0\")\n",
    "    assert data[\"viewstate\"].startswith(\"/wEPDwUKLTI\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = requests.get('http://www.transtats.bts.gov/Data_Elements.aspx?Data=2')\n",
    "soup= BeautifulSoup(r.text)\n",
    "viewstate_element = soup.find(id=\"__VIEWSTATE\")\n",
    "viewstate = viewstate_element['value']\n",
    "eventvalidation_element = soup.find(id='__EVENTVALIDATION')\n",
    "eventvalidation = eventvalidation_element['value']\n",
    "r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                    data={'AirportList': \"BOS\",\n",
    "                          'CarrierList': \"VX\",\n",
    "                          'Submit': 'Submit',\n",
    "                          \"__EVENTTARGET\": \"\",\n",
    "                          \"__EVENTARGUMENT\": \"\",\n",
    "                          \"__EVENTVALIDATION\": eventvalidation,\n",
    "                          \"__VIEWSTATE\": viewstate\n",
    "                    })\n",
    "\n",
    "f = open('virgin_and_logan_airport.html','w')\n",
    "f.write(r.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# s = requests.Session() \n",
    "# r = s.post(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "#            data = (\n",
    "#                    (\"__EVENTTARGET\", \"\"),\n",
    "#                    (\"__EVENTARGUMENT\", \"\"),\n",
    "#                    (\"__VIEWSTATE\", viewstate),\n",
    "#                    (\"__VIEWSTATEGENERATOR\",viewstategenerator),\n",
    "#                    (\"__EVENTVALIDATION\", eventvalidation),\n",
    "#                    (\"CarrierList\", \"VX\"),\n",
    "#                    (\"AirportList\", \"BOS\"),\n",
    "#                    (\"Submit\", \"Submit\")\n",
    "#                   ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Quiz Questions: \n",
    "\n",
    "### Question 1: \n",
    "\"\"\"\n",
    "Your task in this exercise is to modify 'extract_carrier()` to get a list of\n",
    "all airlines. Exclude all of the combination values like \"All U.S. Carriers\"\n",
    "from the data that you return. You should return a list of codes for the\n",
    "carriers.\n",
    "\n",
    "All your changes should be in the 'extract_carrier()' function. The\n",
    "'options.html' file in the tab above is a stripped down version of what is\n",
    "actually on the website, but should provide an example of what you should get\n",
    "from the full file.\n",
    "\n",
    "Please note that the function 'make_request()' is provided for your reference\n",
    "only. You will not be able to to actually use it from within the Udacity web UI.\n",
    "\"\"\"\n",
    "\n",
    "### Question 2\n",
    "\"\"\"\n",
    "Complete the 'extract_airports()' function so that it returns a list of airport\n",
    "codes, excluding any combinations like \"All\".\n",
    "\n",
    "Refer to the 'options.html' file in the tab above for a stripped down version\n",
    "of what is actually on the website. The test() assertions are based on the\n",
    "given file.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "### Question 3 \n",
    "\"\"\"\n",
    "Let's assume that you combined the code from the previous 2 exercises with code\n",
    "from the lesson on how to build requests, and downloaded all the data locally.\n",
    "The files are in a directory \"data\", named after the carrier and airport:\n",
    "\"{}-{}.html\".format(carrier, airport), for example \"FL-ATL.html\".\n",
    "\n",
    "The table with flight info has a table class=\"dataTDRight\". Your task is to\n",
    "use 'process_file()' to extract the flight data from that table as a list of\n",
    "dictionaries, each dictionary containing relevant data from the file and table\n",
    "row. This is an example of the data structure you should return:\n",
    "\n",
    "data = [{\"courier\": \"FL\",\n",
    "         \"airport\": \"ATL\",\n",
    "         \"year\": 2012,\n",
    "         \"month\": 12,\n",
    "         \"flights\": {\"domestic\": 100,\n",
    "                     \"international\": 100}\n",
    "        },\n",
    "         {\"courier\": \"...\"}\n",
    "]\n",
    "\n",
    "Note - year, month, and the flight data should be integers.\n",
    "You should skip the rows that contain the TOTAL data for a year.\n",
    "\n",
    "There are couple of helper functions to deal with the data files.\n",
    "Please do not change them for grading purposes.\n",
    "All your changes should be in the 'process_file()' function.\n",
    "\n",
    "The 'data/FL-ATL.html' file in the tab above is only a part of the full data,\n",
    "covering data through 2003. The test() code will be run on the full table, but\n",
    "the given file should provide an example of what you will get.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "#Question 1\n",
    "html_page = 'Data_Elements.aspx.html'\n",
    "def extract_carriers(page):\n",
    "    data = []\n",
    "\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        carrier_list = soup.find(id ='CarrierList')\n",
    "        for option in carrier_list.find_all('option'): \n",
    "            data.append(option['value'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Question 2 \n",
    "\n",
    "def extract_airports(page): \n",
    "    data = []\n",
    "    with open(page,'r') as html: \n",
    "        soup = BeautifulSoup(html,'lxml')\n",
    "        airport_list = soup.find(id = \"AirportList\")\n",
    "        for option in airport_list.find_all('option'): \n",
    "            data.append(option['value'])\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "datadir = \"data\"\n",
    "\n",
    "\n",
    "def open_zip(datadir):\n",
    "    with ZipFile('{0}.zip'.format(datadir), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def process_all(datadir):\n",
    "    files = os.listdir(datadir)\n",
    "    return files\n",
    "\n",
    "\n",
    "def process_file(f):\n",
    "    \"\"\"\n",
    "    This function extracts data from the file given as the function argument in\n",
    "    a list of dictionaries. This is example of the data structure you should\n",
    "    return:\n",
    "\n",
    "    data = [{\"courier\": \"FL\",\n",
    "             \"airport\": \"ATL\",\n",
    "             \"year\": 2012,\n",
    "             \"month\": 12,\n",
    "             \"flights\": {\"domestic\": 100,\n",
    "                         \"international\": 100}\n",
    "            },\n",
    "            {\"courier\": \"...\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "    Note - year, month, and the flight data should be integers.\n",
    "    You should skip the rows that contain the TOTAL data for a year.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    info = {}\n",
    "    info[\"courier\"], info[\"airport\"] = f[:6].split(\"-\")\n",
    "    # Note: create a new dictionary for each entry in the output data list.\n",
    "    # If you use the info dictionary defined here each element in the list \n",
    "    # will be a reference to the same info dictionary.\n",
    "    with open(\"{}/{}\".format(datadir, f), \"r\") as html:\n",
    "\n",
    "        soup = BeautifulSoup(html)\n",
    "        data_table = soup.find('table',class_ = 'dataTDRight')\n",
    "        \n",
    "        for row in data_table.find_all('tr'): \n",
    "            cells = row.find_all('td')\n",
    "            if cells[1].text == 'Total': \n",
    "                continue \n",
    "            else: \n",
    "                info['courier'] = info['courier']\n",
    "                info['airport'] = info['airport']\n",
    "                info['year'] = cells[0].text\n",
    "                info['month'] = cells[1].text\n",
    "                info[\"flights\"] = {'domestic': str(cells[2].text.replace(',', '')),\n",
    "                                    'international': str(cells[3].text.replace(',', ''))}\n",
    "    print info\n",
    "    return data.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}, {'airport': 0, 'month': 11, 'flights': {'international': 109651, 'domestic': 657708}, 'courier': 0, 'year': 2016}]\n"
     ]
    }
   ],
   "source": [
    "import pprint \n",
    "data = []\n",
    "info = {}\n",
    "with open(html_page, \"r\") as html: \n",
    "    soup = BeautifulSoup(html)\n",
    "    airport_carrier = soup.find('table', class_='dataTDRight')\n",
    "    for row in airport_carrier.find_all('tr')[1:]: #skip the first header row\n",
    "        cells = row.find_all('td')\n",
    "        if cells[1].text == 'TOTAL': \n",
    "            continue\n",
    "        else: \n",
    "            info['courier'] = 0\n",
    "            info['airport'] = 0 \n",
    "            info['year'] = int(cells[0].text)\n",
    "            info['month'] = int(cells[1].text)\n",
    "            info['flights'] = {'domestic': int(str(cells[2].text.replace(',',''))), \n",
    "                                \"international\": int(str(cells[3].text.replace(',','')))}\n",
    "            data.append(info)\n",
    "print data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#regular expression module \n",
    "#xml etree module \n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_file(input_file, output_good, output_bad):\n",
    "    # store data into lists for output\n",
    "    data_good = []\n",
    "    data_bad = []\n",
    "    with open(input_file, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        header = reader.fieldnames\n",
    "        for row in reader:\n",
    "            # validate URI value\n",
    "            if row['URI'].find(\"dbpedia.org\") < 0:\n",
    "                continue\n",
    "\n",
    "            ps_year = row['productionStartYear'][:4]\n",
    "            try: # use try/except to filter valid items\n",
    "                ps_year = int(ps_year)\n",
    "                row['productionStartYear'] = ps_year\n",
    "                if (ps_year >= 1886) and (ps_year <= 2014):\n",
    "                    data_good.append(row)\n",
    "                else:\n",
    "                    data_bad.append(row)\n",
    "            except ValueError: # non-numeric strings caught by exception\n",
    "                if ps_year == 'NULL':\n",
    "                    data_bad.append(row)\n",
    "\n",
    "    # Write processed data to output files\n",
    "    with open(output_good, \"w\") as good:\n",
    "        writer = csv.DictWriter(good, delimiter=\",\", fieldnames= header)\n",
    "        writer.writeheader()\n",
    "        for row in data_good:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    with open(output_bad, \"w\") as bad:\n",
    "        writer = csv.DictWriter(bad, delimiter=\",\", fieldnames= header)\n",
    "        writer.writeheader()\n",
    "        for row in data_bad:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Audit.py \n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "In this problem set you work with cities infobox data, audit it, come up with a\n",
    "cleaning idea and then clean it up. In the first exercise we want you to audit\n",
    "the datatypes that can be found in some particular fields in the dataset.\n",
    "The possible types of values can be:\n",
    "- NoneType if the value is a string \"NULL\" or an empty string \"\"\n",
    "- list, if the value starts with \"{\"\n",
    "- int, if the value can be cast to int\n",
    "- float, if the value can be cast to float, but CANNOT be cast to int.\n",
    "   For example, '3.23e+07' should be considered a float because it can be cast\n",
    "   as float but int('3.23e+07') will throw a ValueError\n",
    "- 'str', for all other values\n",
    "\n",
    "The audit_file function should return a dictionary containing fieldnames and a \n",
    "SET of the types that can be found in the field. e.g.\n",
    "{\"field1\": set([type(float()), type(int()), type(str())]),\n",
    " \"field2\": set([type(str())]),\n",
    "  ....\n",
    "}\n",
    "The type() function returns a type object describing the argument given to the \n",
    "function. You can also use examples of objects to create type objects, e.g.\n",
    "type(1.1) for a float: see the test function below for examples.\n",
    "\n",
    "Note that the first three rows (after the header row) in the cities.csv file\n",
    "are not actual data points. The contents of these rows should note be included\n",
    "when processing data types. Be sure to include functionality in your code to\n",
    "skip over or detect these rows.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "import pprint\n",
    "FIELDS = [\"name\", \"timeZone_label\", \"utcOffset\", \"homepage\", \"governmentType_label\",\n",
    "          \"isPartOf_label\", \"areaCode\", \"populationTotal\", \"elevation\",\n",
    "          \"maximumElevation\", \"minimumElevation\", \"populationDensity\",\n",
    "          \"wgs84_pos#lat\", \"wgs84_pos#long\", \"areaLand\", \"areaMetro\", \"areaUrban\"]\n",
    "fieldtypes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter an integer: 3.2\n",
      "No valid integer! Please try again ...\n",
      "Please enter an integer: 3\n",
      "Great, you successfully entered an integer!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        n = raw_input(\"Please enter an integer: \")\n",
    "        n = int(n)\n",
    "        break\n",
    "    except ValueError:\n",
    "        print(\"No valid integer! Please try again ...\")\n",
    "print \"Great, you successfully entered an integer!\"\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fieldtypes[\"areaLand\"] = set([type(1.1),type([]), type(None)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{float, list, NoneType}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fieldtypes['areaLand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_int(float(\"3.23e+07\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32300000.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float('3.23e+07')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_number(s): \n",
    "    try: \n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError: \n",
    "        return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'areaCode': set([]),\n",
      " 'areaLand': set([]),\n",
      " 'areaMetro': set([]),\n",
      " 'areaUrban': set([<type 'float'>, <type 'NoneType'>, <type 'str'>]),\n",
      " 'elevation': set([]),\n",
      " 'governmentType_label': set([]),\n",
      " 'homepage': set([]),\n",
      " 'isPartOf_label': set([]),\n",
      " 'maximumElevation': set([]),\n",
      " 'minimumElevation': set([]),\n",
      " 'name': set([]),\n",
      " 'populationDensity': set([]),\n",
      " 'populationTotal': set([]),\n",
      " 'timeZone_label': set([]),\n",
      " 'utcOffset': set([]),\n",
      " 'wgs84_pos#lat': set([]),\n",
      " 'wgs84_pos#long': set([])}\n"
     ]
    }
   ],
   "source": [
    "fieldtypes = {}\n",
    "for field in FIELDS: \n",
    "    fieldtypes[field] = set() \n",
    "with open('cities.csv','r') as f: \n",
    "    reader = csv.DictReader(f)\n",
    "    for i in range(3): \n",
    "        reader.next()\n",
    "    for row in reader: \n",
    "        for fields in FIELDS: \n",
    "            if row[fields] == 'NULL' or row[fields] == \"\" or row[fields] == None: \n",
    "                fieldtypes[field].add(type(None))\n",
    "            if row[fields] == \"{\": \n",
    "                fieldtypes[field].add(type([]))\n",
    "            else: \n",
    "                try: \n",
    "                    int(row[field])\n",
    "                    fieldtypes.add(type(1))\n",
    "                except ValueError: \n",
    "                    try: \n",
    "                        float(row[field])\n",
    "                        fieldtypes[field].add(type(1.1))\n",
    "                    except ValueError:\n",
    "                        fieldtypes[field].add(type('string'))\n",
    "pprint.pprint(fieldtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row[fields].find('}') # zero if found -1 otherwise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audit_file(filename, fields):\n",
    "    fieldtypes = {}\n",
    "    for fields in FIELDS: \n",
    "        fieldtypes[fields] = set() # make a set for the fields\n",
    "    with open(filename,'r') as f: \n",
    "        reader = csv.DictReader(f) # create a dictionary reader\n",
    "        for i in range(3): \n",
    "            reader.next() # skip the first three files \n",
    "        for row in reader: \n",
    "            for fields in FIELDS: \n",
    "                if row[fields] == \"NULL\" or row[fields] == \"\": \n",
    "                    fieldtypes[fields].add(type(None)) \n",
    "                elif row[fields].find(\"{\") == 0: \n",
    "                    fieldtypes[fields].add(type([]))\n",
    "                else:\n",
    "                    try: \n",
    "                        int(row[fields]) \n",
    "                        fieldtypes[fields].add(type(1)) \n",
    "                    except ValueError: \n",
    "                        try: \n",
    "                            float(row[fields])\n",
    "                            fieldtypes[fields].add(type(1.1))\n",
    "                        except ValueError: \n",
    "                            fieldtypes[fields].add(type('string')) \n",
    "    return fieldtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'areaCode': {int, NoneType, str},\n",
       " 'areaLand': {float, list, NoneType},\n",
       " 'areaMetro': {float, NoneType},\n",
       " 'areaUrban': {float, NoneType},\n",
       " 'elevation': {float, list, NoneType},\n",
       " 'governmentType_label': {NoneType, str},\n",
       " 'homepage': {NoneType, str},\n",
       " 'isPartOf_label': {list, NoneType, str},\n",
       " 'maximumElevation': {NoneType},\n",
       " 'minimumElevation': {NoneType},\n",
       " 'name': {list, NoneType, str},\n",
       " 'populationDensity': {float, list, NoneType},\n",
       " 'populationTotal': {int, NoneType},\n",
       " 'timeZone_label': {NoneType, str},\n",
       " 'utcOffset': {int, list, NoneType, str},\n",
       " 'wgs84_pos#lat': {float},\n",
       " 'wgs84_pos#long': {float}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audit_file('cities.csv',FIELDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{1.20175e+07|1.202e+07}'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['areaLand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fix_area(area):\n",
    "    data_area = []\n",
    "    if area == 'NULL': \n",
    "        return None\n",
    "    else: \n",
    "        area = area.replace('{',\"\")\n",
    "        area = area.replace('}',\"\")\n",
    "        split_area = area.split(\"|\")\n",
    "        for num in split_area: \n",
    "            data_area.append(float(num))\n",
    "        return data_area\n",
    "\n",
    "def process_file(filename):\n",
    "    # CHANGES TO THIS FUNCTION WILL BE IGNORED WHEN YOU SUBMIT THE EXERCISE\n",
    "    data = []\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "\n",
    "        #skipping the extra metadata\n",
    "        for i in range(3):\n",
    "            l = reader.next()\n",
    "\n",
    "        # processing file\n",
    "        for line in reader:\n",
    "            # calling your function to fix the area value\n",
    "            if \"areaLand\" in line:\n",
    "                line[\"areaLand\"] = fix_area(line[\"areaLand\"])\n",
    "            data.append(line)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = process_file('cities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area = data[6]['areaLand'].replace(\"{\", \"\")\n",
    "area = area.replace(\"}\",\"\")\n",
    "split_area = area.split(\"|\")\n",
    "data_area = []\n",
    "for i in split_area: \n",
    "    data_area.append(float(i))\n",
    "len(data_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[55166700.0, 55300000.0]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[8]['areaLand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{1.20175e+07|1.202e+07}'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['areaLand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = ET.parse('example.osm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Children of root:\n",
      "'bounds'\n",
      "'node'\n",
      "'node'\n",
      "'node'\n",
      "'node'\n",
      "'node'\n",
      "'node'\n",
      "'node'\n",
      "'node'\n",
      "'node'\n",
      "'node'\n",
      "'node'\n",
      "'node'\n",
      "'node'\n",
      "'node'\n",
      "'node'\n",
      "'node'\n",
      "'node'\n",
      "'node'\n",
      "'node'\n",
      "'node'\n",
      "'way'\n",
      "'relation'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get its root \n",
    "root = tree.getroot() \n",
    "tags = {'bounds': 0,\n",
    "        \"member\": 0,\n",
    "        'nd': 0,\n",
    "        \"node\": 0, \n",
    "        \"osm\": 0,\n",
    "        \"relation\": 0,\n",
    "        \"tag\": 0,\n",
    "        \"way\": 0,\n",
    "       }\n",
    "# print the object \n",
    "print \"\\nChildren of root:\"\n",
    "for child in root: \n",
    "    pprint.pprint(child.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 0,\n",
      " 'member': 0,\n",
      " 'nd': 0,\n",
      " 'node': 0,\n",
      " 'osm': 0,\n",
      " 'relation': 0,\n",
      " 'tag': 0,\n",
      " 'way': 0}\n",
      "{'bounds': 0,\n",
      " 'member': 0,\n",
      " 'nd': 0,\n",
      " 'node': 20,\n",
      " 'osm': 0,\n",
      " 'relation': 0,\n",
      " 'tag': 0,\n",
      " 'way': 0}\n",
      "{'bounds': 0,\n",
      " 'member': 0,\n",
      " 'nd': 0,\n",
      " 'node': 20,\n",
      " 'osm': 0,\n",
      " 'relation': 0,\n",
      " 'tag': 0,\n",
      " 'way': 0}\n",
      "{'bounds': 0,\n",
      " 'member': 0,\n",
      " 'nd': 0,\n",
      " 'node': 20,\n",
      " 'osm': 0,\n",
      " 'relation': 0,\n",
      " 'tag': 0,\n",
      " 'way': 0}\n",
      "{'bounds': 0,\n",
      " 'member': 0,\n",
      " 'nd': 0,\n",
      " 'node': 20,\n",
      " 'osm': 0,\n",
      " 'relation': 0,\n",
      " 'tag': 0,\n",
      " 'way': 1}\n",
      "{'bounds': 0,\n",
      " 'member': 0,\n",
      " 'nd': 0,\n",
      " 'node': 20,\n",
      " 'osm': 0,\n",
      " 'relation': 1,\n",
      " 'tag': 0,\n",
      " 'way': 1}\n",
      "{'bounds': 0,\n",
      " 'member': 0,\n",
      " 'nd': 0,\n",
      " 'node': 20,\n",
      " 'osm': 0,\n",
      " 'relation': 1,\n",
      " 'tag': 0,\n",
      " 'way': 1}\n",
      "{'bounds': 1,\n",
      " 'member': 0,\n",
      " 'nd': 0,\n",
      " 'node': 20,\n",
      " 'osm': 0,\n",
      " 'relation': 1,\n",
      " 'tag': 0,\n",
      " 'way': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for key in tags:\n",
    "    results = root.findall(key)\n",
    "    \n",
    "    pprint.pprint(tags)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bounds': 1,\n",
       " 'member': 0,\n",
       " 'nd': 0,\n",
       " 'node': 20,\n",
       " 'osm': 0,\n",
       " 'relation': 1,\n",
       " 'tag': 0,\n",
       " 'way': 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list = []\n",
    "for i in tags: \n",
    "    list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['member', 'node', 'tag', 'osm', 'way', 'relation', 'nd', 'bounds']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
